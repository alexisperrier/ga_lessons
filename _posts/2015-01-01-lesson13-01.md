---
layout: slide
title: Lesson 13 - Natural Language Processing
description: none
transition: slide
permalink: /13-nlp.html
theme: ga
---

# Review: SVM Decision Trees and Random Forests

# Natural Language Processing: NLP

### HUGE DOMAIN

* Classify documents,
* Predict:
* Detect (depression, ...)
* Translate, Summarize, ....
* Named entity recognition
* Survey analysis
* Automatic Speech Recognition (Siri, Alexa, ...)
* Unsupervised: infer sentiment or topics, find associations and links,

# Tokenize

Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.


# Vectorizing text

First we need to transform the raw text into vectors of numerical values

Extract numerical features from text

* tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.
* counting the occurrences of tokens in each document.
* normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.

# Text feature extraction

Features and samples are defined as follows:

* each individual token occurrence frequency (normalized or not) is treated as a feature.
* the vector of all the token frequencies for a given document is considered a multivariate sample.
* A corpus of documents can thus be represented by
    * a matrix with one row per document
    * one column per token (e.g. word) occurring in the corpus.

# Bag of Words

Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.

This approach (tokenization, counting and normalization) is called the **Bag of Words** or **Bag of n-grams**representation.

# NLP Vocabulary

* Corpus: ensemble of documents
* Token: the element, the word, the atom
* grams, mono-grams, bi-grams, n-grams: sequence of 1, 2 or n words taken as the basic element
* Stopwords: small words that are discarded as not meaningful: a an the my get ...

# Lemmatization and Stemming

# POS - Tagging

# Libraries

* Scikit
* NLTK
* Spacy (POS)
* Gensim and LDA (Latent Dirichlet Allocation - Topic Modeling)

# Demo 1: Vectorizing a text

[Vectoring a text](https://github.com/alexperrier/gads/blob/master/13-nlp/py/L13%20N1%20Feature%20Extraction.ipynb)

# Normalizing: TF-IDF

What if you have several documents and a word is very frequent in just one of them.
It's relative frequency should not be important.

TF-IDF: term frequency–inverse document frequency

TF:
Boolean "frequencies": tf(t,d) = 1 if t occurs in d and 0 otherwise;
logarithmically scaled frequency: tf(t,d) = 1 + log ft,d, or zero if ft,d is zero;
augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:
{\displaystyle \mathrm {tf} (t,d)=0.5+0.5\cdot {\frac {f_{t,d}}{\max\{f_{t',d}:t'\in d\}}}}

inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents

 the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.

  \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}

N: total number of documents in the corpus {\displaystyle N={|D|}} N = {|D|}
{\displaystyle |\{d\in D:t\in d\}|}  |\{d \in D: t \in d\}|  : number of documents where the term {\displaystyle t} t appears (i.e., {\displaystyle \mathrm {tf} (t,d)\neq 0}  \mathrm{tf}(t,d) \neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\displaystyle 1+|\{d\in D:t\in d\}|} 1 + |\{d \in D: t \in d\}|

Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency. This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results), that has also found good use in document classification and clustering.

Then tf–idf is calculated as
{\displaystyle \mathrm {tfidf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)} {\displaystyle \mathrm {tfidf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)}

### in Scikit
[TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)

# Hashing Trick

* Documents don't have the same length
* You may not have all the documents available (streaming)
* Too many words => too many dimensions
* highly dimensional very sparse input matrix.

=> Dimensionality reduction with **The Hashing Trick**


http://blog.someben.com/2013/01/hashing-lang/
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html
http://www.shogun-toolbox.org/static/notebook/current/HashedDocDotFeatures.html

at the expense of collisions and interpretability

# Install a few things

conda install BeautifulSoup4
nltk

# 1st Lab predicting sentiment
https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words

[N2 Imdb Reviews](https://github.com/alexperrier/gads/blob/master/13-nlp/py/L13%20N2%20Sentiment%20Prediction.ipynb)

* get the data
* clean up the text: remove punctuation, html markup, numbers, stop words, tokenize
    and return one long paragraph per document
* process all the reviews, store into an array
* train a RF
* assess on the test set
* AUC curve

Use scikit CountVectorizer and Try to improve the score

# Pipeline in scikit

The ability to chain operations

for instance

* standard scaler
* classifier

Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.
The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below.


# 2nd lab: classification




http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html

# 3rd Clustering
with HashingVectorizer
Silhouette Coefficient
Cosine similarity

http://scikit-learn.org/stable/auto_examples/text/document_clustering.html



