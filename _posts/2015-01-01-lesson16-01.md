---
layout: slide
title: Lesson 16, 17 - Time Series
description: none
transition: slide
permalink: /16-time-series.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>16. 17. Time series</p>
</section>

<section data-markdown>
# Previously

* Topic Modeling
* LDA, LSA
* Gensim, NLTK

</section>

<section data-markdown>
# Today

![](assets/16/time-series-analysis.png)

* Time series
* Modeling
* Stationarity
* Treding and seasonality
* Forecasting 101

</section>
<section data-markdown>
# Time Series [TS]

A time series is a series of data points listed in time order.

A sequence taken at successive equally spaced points in time.

A sequence of **discrete-time data**.

=> The **time interval** is key

</section>
<section data-markdown>
# Time Series [TS]

* **IoT**
* **signal processing**
* pattern recognition
* **econometrics**
* mathematical finance / trading
* intelligent transport and trajectory forecasting
* weather forecasting and Climate change research
* earthquake prediction, astronomy
* electroencephalography, control engineering, communications

* Any domain of applied science and engineering which involves temporal measurements.

</section>

<section data-markdown>
# TS

* Modeling
* Forecasting
* Signal detection
* Detection of a change in the parameters of a static or dynamic system

</section>

<section data-markdown>
# Time vs Frequency domain: Fourier Transform

The Fourier transform decomposes a function of time (a signal) into the frequencies that make it up. A Fourier transform takes a time series or a function of continuous time, and maps it into a frequency spectrum.

* Jean-Baptiste Joseph Fourier, 1807 Treatise on the *propagation of heat in solid bodies*.

![](assets/16/sFFT.png)

</section>

<section data-markdown>
# TS: Components

* [Trend](https://datamarket.com/data/set/22s6/weekly-closing-price-of-att-common-shares-1979#!ds=22s6&display=line):
    * Gradual long term evolution
    * Easiest to detect

* [Cycle](https://datamarket.com/data/set/235d/mean-daily-temperature-fisher-river-near-dallas-jan-01-1988-to-dec-31-1991#!ds=235d&display=line) or Seasonal Variation
    * Up and down repetitive movement
    * Repeats itself over a long period of time

* Random Variations
    * Erratic movements that do not follow a pattern
    * Not predictable

Examples:

* https://datamarket.com/data/set/22ox/monthly-milk-production-pounds-per-cow-jan-62-dec-75#!ds=22ox&display=line
* https://datamarket.com/data/set/22vd/quarterly-production-of-clay-bricks-million-units-mar-1956-sep-1994#!ds=22vd&display=line
* https://datamarket.com/data/set/235d/mean-daily-temperature-fisher-river-near-dallas-jan-01-1988-to-dec-31-1991#!ds=235d&display=line

</section>

<section data-markdown>
# Loading and indexing in Pandas

Simple loading

        ts = pd.read_csv('../data/Dow-Jones.csv', parse_dates = ['Date'], infer_datetime_format = True)
        ts[ts.Date > '2010-01-01']

Make the date the index:

        ts = pd.read_csv('../data/Dow-Jones.csv', parse_dates=['Date'], index_col='Date', infer_datetime_format = True)
        ts['2010-12-31':'2010-01-01']
        ts[:'2010-01-01']


</section>

<section data-markdown>
# Forecasting 101

* Smoothing the data
* Simple forecasting technique
* Tree rings dataset

        ts = pd.read_csv('../data/tree-rings.csv', parse_dates = ['year'], index_col = 'year', infer_datetime_format = True)

</section>
<section data-markdown>
# plot the tree rings

[Notebook - Smoothing and Forecast 101]()

</section>

<section data-markdown>
# Very simple forecasting technique

$$ \hat{Y}\_{n+1} = Y\_n $$

* load the ts
* create a new column 1 period gap

How good is that predictor?
</section>

<section data-markdown>
# Ts metrics

Forecasting error: \\( e\_i=y\_i−\hat{y}\_i \\)


Metrics to compare TS techniques

* Mean Absolute Error: \\( MAE=mean(|e\_i|) \\)
* [Mean Absolute Deviation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mad.html): \\( MAD = \frac{1}{n} \sum\_{i=1}^{n}  | e\_i |  \\)
* Root mean squared error: \\( RMSE = \sqrt{  mean(e\_i^2)  } \\)
* Mean absolute percentage error: \\( MAPE = \frac{100}{n} \sum\_{i=1}^{n} \frac{ | e\_i | }{ y\_i } \\)

</section>

<section data-markdown>
# Moving Average


Smoothed over a window of n samples

$$ \begin{aligned} SMA = \frac{p\_{M} + p\_{M-1}+ \cdots +p\_{M-(n-1)}}{n} = \frac{1}{n} \sum\_{i=0}^{n-1}p\_{M-i}
\end{aligned} $$

and center

$$ \begin{aligned} SMA = \frac{p\_{M+n/2} + \cdots +   p\_{M+1} +   p\_{M} + p\_{M-1}+ \cdots +p\_{M-(n/2)}}{n} = \frac{1}{n} \sum\_{i=-\frac{n}{2}}^{\frac{n}{2}}p\_{M+i}
\end{aligned} $$



</section>



<section data-markdown>
# Moving Average
Use SMA to forecast

</section>

<section data-markdown>
# Exponential weighted moving average


![](assets/16/exponential_moving_average_weights.png)

</section>

<section data-markdown>
# Exponential weighted moving average
Introduce a Decay

The EWMA for a series Y may be calculated recursively:

* \\( S\_{1}=Y\_{1} \\)
* \\(      t>1, \ \ S\_{t}=\alpha \cdot Y\_{t}+(1-\alpha )\cdot S\_{t-1} \\)


Where:

* The coefficient α represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher α discounts older observations faster.
* Yt is the value at a time period t.
* St is the value of the EMA at any time period t.

http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.ewma.html
</section>

<section data-markdown>

# Auto Correlation

A measure of how much is the current value influenced by the previous values in a time series.

Autocorrelation measures the linear relationship between lagged values of a time series.

        pandas autocorrelation_plot()
</section>

<section data-markdown>

# Partial Auto Correlation

Let's say you have a TS with a high correlation 1 sample apart

* 1) \\( Y\_{n+1}  \\) very correlated with \\( Y\_{n}  \\)
* 2) \\( Y\_{n}  \\) very correlated with \\( Y\_{n-1}  \\)
* etc ...

That correlation impacts the correlation between samples  that are further apart

* \\( Y\_{n+1}  \\) appears very correlated with \\( Y\_{n-1}  \\) since 1_ and 2)

The partial autocorrelation function (PACF) can be thought of as the correlation between two points that are separated by some number of periods n, BUT with the effect of the intervening correlations removed.

The PACF removes the intervening correlation between the samples

</section>

<section data-markdown>
# Statsmodels
http://statsmodels.sourceforge.net/stable/tsa.html

</section>

<section data-markdown>
# Properties: Stationarity

The concept of **stationarity** is very important in modeling non-independent data

Many results which holds for independent random variables (law of large numbers, central limit theorem, ...) hold for stationary random variables.


A time serie is said to be stationnary if

* **Constant mean**: The mean of the series should not be a function of time rather should be a constant.

* **Constant variance**: Homoscedasticity: The variance of the series should not a be a function of time.

* **Fix lagged covariance**: The covariance of the i th term and the (i + m) th term should only depend on i and not m.

</section>

<section data-markdown>
# Properties: Stationarity
**Constant mean**

![](assets/16/Mean_nonstationary.png)
</section>

<section data-markdown>
# Properties: Stationarity

**Constant variance**

![](assets/16/Var_nonstationary.png)
</section>

<section data-markdown>
# Properties: Stationarity
**Fix lagged covariance**

![](assets/16/Cov_nonstationary.png)
</section>

<section data-markdown>
# Properties: Stationarity
* Tree rings?
* Dow Jones?
* Average Water Temp?


</section>

<section data-markdown>
# Stationarity test

* Dickey-Fuller Test: This is one of the statistical tests for checking stationarity.
* Here the null hypothesis is that the TS is non-stationary.

* The test results comprise of a Test Statistic and some Critical Values for difference confidence levels.

* If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.

    For instance: the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence.

</section>

<section data-markdown>
# Dickey Fuller test

* from statsmodels.tsa.stattools import adfuller
* dftest = adfuller(timeseries, autolag='AIC')

Returns: ['Test Statistic','p-value','#Lags Used','Number of Observations Used']

</section>

<section data-markdown>
# How to make a TS stationary
There are 2 major reasons behind non-stationaruty of a TS:
1. Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.
2. Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.

so remove trend and seasonality
and apply prediction on resulting TS

</section>

<section data-markdown>
# Trend

Estimating & Eliminating Trend

One of the first tricks to reduce trend can be transformations such as log, square root, cube root, etc. Lets take a log transform here for simplicity:

### Notebook

* Load the [Monthly milk production](https://datamarket.com/data/set/22sn/monthly-milk-production-pounds-per-cow-jan-62-dec-75-adjusted-for-month-length#!ds=22sn&display=line) dataset
* Fit a linear regression line
* Remove Moving Average
* Fit a linear regression line
* plot before after

</section>

<section data-markdown>
# Decomposing
both trend and seasonality are modeled separately

        from statsmodels.tsa.seasonal import seasonal_decompose
        decomposition = seasonal_decompose(ts_log)

        trend = decomposition.trend
        seasonal = decomposition.seasonal
        residual = decomposition.resid

        # Then plot
        plt.subplot(411)
        plt.plot(ts_log, label='Original')
        plt.legend(loc='best')
        plt.subplot(412)
        plt.plot(trend, label='Trend')
        plt.legend(loc='best')
        plt.subplot(413)
        plt.plot(seasonal,label='Seasonality')
        plt.legend(loc='best')
        plt.subplot(414)
        plt.plot(residual, label='Residuals')
        plt.legend(loc='best')
        plt.tight_layout()

</section>
<section data-markdown>
# Properties: Seasonality

</section>

<section data-markdown>
# Transformations
https://www.otexts.org/fpp/2/4

* Mathematical: Box plot
* Calendar Adjustments
    monthly milk production on a farm,

* Population adjustments
    per 1000
    you remove the effect of population changes by considering number of beds per thousand people
* Inflation adjustments
        for money related series
         a price index is used. If ztzt denotes the price index and ytyt denotes the original house price in year tt, then xt=yt/zt∗z2000xt=yt/zt∗z2000 gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).


</section>

<section data-markdown>
# Differencing
Differencing – taking the differece with a particular time lag

One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity.

</section>

<section data-markdown>
# Decomposing
both trend and seasonality are modeled separately
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()

<!-- Part II: Modeling -->
</section>

<section data-markdown>
# Naive Method
Naïve method
This method is only appropriate for time series data. All forecasts are simply set to be the value of the last observation. That is, the forecasts of all future values are set to be yTyT, where yTyT is the last observed value. This method works remarkably well for many economic and financial time series.

=>

</section>

<section data-markdown>
# Forecasting with Moving average

=> look at residuals for different m
=> RMSE, vs MAE, vs MAPE for different Ms

</section>

<section data-markdown>
# Drift method
Drift method
A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. So the forecast for time T+hT+h is given by

yT+hT−1∑t=2T(yt−yt−1)=yT+h(yT−y1T−1).
yT+hT−1∑t=2T(yt−yt−1)=yT+h(yT−y1T−1).
This is equivalent to drawing a line between the first and last observation, and extrapolating it into the future.

</section>

<section data-markdown>
# AR
</section>

<section data-markdown>
# MA
</section>

<section data-markdown>
# ARMA
</section>

<section data-markdown>
# ARIMA

</section>

<section data-markdown>
# De treding
</section>

<section data-markdown>
# De seasonality
</section>

<section data-markdown>
# Predicting
</section>

<section data-markdown>
# Cross validation

Training and tests sets
</section>

<section data-markdown>
# one-step forecasts may not be as relevant as multi-step forecasts

</section>

<section data-markdown>
# Residuals
https://www.otexts.org/fpp/2/6
A good forecasting method will yield residuals with the following properties:

The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

The residuals have constant variance.
The residuals are normally distributed.

Histograms of residuals
QQ plots
ACF of the residuals
Box-Pierce test
Ljung-Box

</section>

<section data-markdown>
# Prediction intervals
https://www.otexts.org/fpp/2/7

https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.tsplot.html

</section>

<section data-markdown>
# Links

* [fecon235 : Computational data tools for financial economics](https://github.com/rsvp/fecon235)
* [Seasonal ARIMA with Python](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/)
* [Complete guide to create a Time Series Forecast ](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)
* [A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)
* [A Simple Time Series Analysis Of The S&P 500 Index](http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/)
* [Identifying the order of differencing in an ARIMA model](http://people.duke.edu/~rnau/411arim2.htm)

* [Time Series Analysis using iPython](https://bicorner.com/2015/11/16/time-series-analysis-using-ipython/)
Others:
* https://www.otexts.org/fpp/2/5
* http://slideplayer.com/slide/8130770/ slide 10

</section>

<section data-markdown>

# White noise
White noise
Time series that show no autocorrelation are called "white noise". Figure 2.11 gives an example of a white noise series.

white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance;

A necessary (but, in general, not sufficient) condition for statistical independence of two variables is that they be statistically uncorrelated; that is, their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element Rii is the variance of component wi; and the correlation matrix must be the n by n identity matrix.

Being uncorrelated in time does not restrict the values a signal can take. Any distribution of values is possible (although it must have zero DC component). Even a binary signal which can only take on the values 1 or -1 will be white if the sequence is statistically uncorrelated. Noise having a continuous distribution, such as a normal distribution, can of course be white.

It is often incorrectly assumed that Gaussian noise (i.e., noise with a Gaussian amplitude distribution — see normal distribution) necessarily refers to white noise, yet neither property implies the other. Gaussianity refers to the probability distribution with respect to the value, in this context the probability of the signal falling within any particular range of amplitudes, while the term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies.

We can therefore find Gaussian white noise, but also Poisson, Cauchy, etc. white noises. Thus, the two words "Gaussian" and "white" are often both specified in mathematical models of systems. Gaussian white noise is a good approximation of many real-world situations and generates mathematically tractable models. These models are used so frequently that the term additive white Gaussian noise has a standard abbreviation: AWGN.

White noise is the generalized mean-square derivative of the Wiener process or Brownian motion.

E[\varepsilon _{t}]=0\,,
{\displaystyle E[\varepsilon _{t}^{2}]=\sigma ^{2}\,,} E[\varepsilon _{t}^{2}]=\sigma ^{2}\,,
{\displaystyle E[\varepsilon _{t}\varepsilon _{s}]=0\quad \forall t\not =s\,.} {\displaystyle E[\varepsilon _{t}\varepsilon _{s}]=0\quad \forall t\not =s\,.}


usually we assume the samples all follow a Gaussian distribution
samples = numpy.random.normal(mean, std, size=num_samples)

</section>
