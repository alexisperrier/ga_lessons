---
layout: slide
title: Lesson 10 - Unsupervised Learning
description: none
transition: slide
permalink: /10-unsupervised-learning.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>10. Unsupervised Learning </p>
</section>

<section data-markdown>
# Lesson 10
## LEARNING OBJECTIVES

* K-Means
* PCA

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #10</h1>
    <p class = 'big_title'>Review of Lesson 9</p>
</section>


<section data-markdown>
# Last Lesson Review

* Why not use a simple linear regression to predict classification?
* Binomial Logistic Regression
* Transformations in Scikit
* Recap on encoding categorical values

</section>

<section data-markdown>
# Last session
### Any Questions?

* What's the sigmoid function?
* Odds ratio
* ...


Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Unsupervised Learning</p>
</section>


<section data-markdown>


# Unsupervised Learning

Not interested in predictions

We only have observations

Can we discover patterns or subgroups, visualize the observations ?

**PCA**: Principal Compenent analysis for data visualization or data preprocessing

**Clustering**: for discovering sub groups in the data
</section>

<section data-markdown>
# Challenges

* Often performed part of exploratory analysis
* Subjective, no way to check
* Difficult to assess the result
* No labels

</section>

<section data-markdown>
# Clustering

Looks for homogeneous subgroups in the data

Market segmentation

2 most common methods

* K-Means clustering: predefined number of clusters
* Hierarchical clustering:

</section>

<section data-markdown>
# K-Means Clustering

The number of clusters K is abitrary.

Let \\( C_k  \text{for} k \in [1,..,K] \\) be the number of clusters. We make 2 assumptions:

* All observations belong to at least one cluster \\( C_k\\)
* No observations belong to 2 clusters

And define good clustering as clustering for which the *within-cluster* variation is as small as possible.

We want each clusters to have minimal variation between the observations that compose it.

</section>

<section data-markdown>
# K-Means Clustering
A more formal definition is

*The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. *

**Inertia**: \\( \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_j - \mu_i||^2) \\)

</section>

<section data-markdown>
# K-Means Clustering

Choose K arbitrarily

1. [Initialize] randomly assign a number 1 to K to each sample
2. Iterate until convergence
    1. For all clusters, calculate the center of all the points in the cluster.
        This creates K points called centroids.
    2. Then reassign all the observations to the cluster that has the closest centroid

Using the Euclidian distance between points.

</section>

<section data-markdown>
# K-Means Clustering: challenges
We have a metric! we can select the *Best* clustering! LOL

* Dependent on the initialization => must run the clustering with several initializations
* Until convergence can mean no more changes or very little difference between iterations
* Choosing the number of clusters K is not an easy problem
* Should you normalize your data?
* In the end are you clustering the noise?
* Will outliers skew your clusters?
* Sensitive to perturbations!

</section>

<section data-markdown>
# Lab

![](assets/10/cluster_datasets.png)

</section>

<section data-markdown>
# Lab

Make these 4 datasets:

        n_samples = 1500
        noisy_circles   = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)
        noisy_moons     = datasets.make_moons(n_samples=n_samples, noise=.05)
        blobs           = datasets.make_blobs(n_samples=n_samples, random_state=8)
        no_structure    = np.random.rand(n_samples, 2), None

and check how these 3 algorithms perform on each

        kmeans = cluster.KMeans(n_clusters=2)
        dbscan = cluster.DBSCAN(eps=.2)
        spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity="nearest_neighbors")

Use
        colors = np.hstack([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'] * 20)
        X, y = dataset
        X = StandardScaler().fit_transform(X)
        cluster.fit(X)
        y_pred = cluster.labels_.astype(np.int)
        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)


</section>

<section data-markdown>
# Examples

scikit http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#example-cluster-plot-kmeans-assumptions-py

Vector Quantization
http://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html#example-cluster-plot-face-compress-py

Excellent and thorough article http://inseaddataanalytics.github.io/INSEADAnalytics/Report_s45.html

</section>

<section data-markdown>
# part II PCA

### Eigenvalue decomposition of the covariance matrix

* X data (n x p) n samples, p features
* Covariance Matrix: X. X^T => n * n matrix => square => can be decomposed
* There exists unique matrixes W and Lambda  X.X^T = W . Lambda . W^T
where Lambda is diagonal (eigenvalues) and W is triangular composed of the eigen vectors

lambda is an eigenvalue of a matrix A iff A.v = lambda.v


=> all the vectors in W are decorrelated!!!

</section>

<section data-markdown>
# PCA Geometric interpretation

* 1st component (vector) gets the max of the variance in the data
* 2nd gets the rest
* 3rd gets the rest ....

and all these vectors are orthogonal


* The data is projected first on the 1st vector => 1st element of the decomposition
* Then on the 2nd direction => 2nd element

</section>

<section data-markdown>
# PCA For dimensionality reduction

Not all lambdas are equal ... in fact the tend to decrease rather fast

</section>

<section data-markdown>
# Demo

Find how many eigenvalues do you need ot keep to retain 80% of the variation in the data?

Caravan dataset

        df = pd.read_csv('../../datasets/Caravan.csv', index_col = False)
        X = df.values
        scale = StandardScaler()
        X = scale.fit_transform(X)

        # Covariance matrix
        covmat = X.T.dot(X)

        # Eigenvalues
        from scipy import linalg
        evs, evmat = linalg.eig(covmat)
        evs = evs.astype(float)

</section>

<section data-markdown>
# LAB 2

And now with scikit PCA

http://nbviewer.jupyter.org/github/JWarmenhoven/ISLR-python/blob/master/Notebooks/Chapter%2010.ipynb

</section>


