---
layout: slide
title: Lesson 11 - SVM and Unbalanced datasets
description: none
transition: slide
permalink: /11-svm-unbalanced.html
theme: ga
---

<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>11. Support Vector Machines and imbalanced datasets</p>
</section>

<section data-markdown>
# Lesson 11
## LEARNING OBJECTIVES

* SVM
* Imbalanced datasets

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #11</h1>
    <p class = 'big_title'>Review of Lesson 10</p>
</section>


<section data-markdown>
# Last Lesson Review

* K-Means
* PCA

</section>


<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Support Vector Machines</p>
</section>


<section data-markdown>


# Hyperplane

Classification separating an hyperplane

* 1D => point
* 2D => line
* 3D => surface
* etc ...

A hyperplane in 2 dimension is defined by a line equation \\( \beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0  \\)


In **p dimensions** a hyperplane is defined by: $$\beta_0 + \beta_1 X_1 + ... + \beta_p X_p = 0 \qquad  p \gt 0 $$

</section>

<section data-markdown>

# Linearly separable

We have n samples in p dimensions/features  \\( X = \\{ x_{i}  \\}  \quad i \in [1,n]  \\).

These observations belong to two classes \\( y_i \in \\{-1, +1 \\} \\)

The classes are **linearly separable**: there is an  hyperplane that fully separates the points according to their classes.

All points \\( x_i \\) such that

* \\( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  < 0 \\) belong to class -1
* \\( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  > 0 \\) belong to class +1

</section>

<section data-markdown>
# Linear decision boundary
We predict the class of a new point \\( x^{\prime} \\)

by calculating
$$ f(x^{\prime}) = \beta\_0+ \sum\_{j=1}^{n} \beta\_j x\_{j}^{\prime}  $$

* \\( f(x^{\prime}) >0 => +1 \\)
* \\( f(x^{\prime}) <0 => -1 \\)

A classifier that is based on a separating hyperplane leads to a **linear decision boundary**.

</section>

<section data-markdown>
# One line

Note that since

* \\( y\_i \\)
* \\( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j}  \\)

have the same sign

$$ y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > 0 $$

</section>

<section data-markdown>
# Infinity of Hyperplanes

If our data is linearly separable then there exists an infinity of hyperplanes that can separate it

![](assets/11/hyperplane-classification.png)


=> We need to introduce a margin to separate the classes

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)
</section>

<section data-markdown>
# Maximal Margin Classifier

We want to find the Hyperplane that will maximise the distance to all the points.
Best separation of the classes

![Maximal Margin Classifier](assets/11/Maximal_Margin_Classifier.png)

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)
</section>

<section data-markdown>
# Maximal Margin Classifier

Finding the largest margin that separates the classes is equivalent to solving

$$ \max\_{\beta\_i} M \quad \text{such that} \quad y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > M \quad \forall i \in [1,..,n] $$

with \\( \sum\_{j=0}^{p}  \beta^2\_j = 1\\)

</section>

<section data-markdown>
# Support Vectors

The observations that are on the margin lines are called **support vectors**

They *support* the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyper- plane would move as well.

Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations: a movement to any of the other observations would not affect the separating hyperplane, provided that the observationâ€™s movement does not cause it to cross the boundary set by the margin.

A change in the support vector impacts the margin a lot => over fitting

</section>

<section data-markdown>
# Support Vector Classifier

What if we allow some points to be either wrongly classified or at least within the margin boundaries?

This is the case when the classes are not separable, we still want the best margin possible. But some points will be on the wrong side

Add a tuning parameter C that dictates the severity of the violation of the margin

The bigger C is the more points are

* within the margin
* or misclassified

C is some budget for violation of the margin which is determined during cross validation

Even in the case of linearly separable classes adding some flexibility to the margin will make the classfier more robust, more flexible (less overfitting)


</section>

<section data-markdown>
# Support Vector Classifier

\\( C\\) is a non negative tuning parameter

$$ \max\_{\beta\_i, \epsilon\_i} M \quad \text{such that} \quad y\_i ( \beta\_0 + \sum\_{j=1}^{n} \beta\_j x\_{i,j} ) > M - \epsilon\_i \quad \forall i \in [1,..,n]  $$
with

* \\( \sum\_j  \beta^2\_j = 1 \\)
* \\(   \epsilon\_i > 0 \\)
* \\( \sum\_{i} \epsilon\_i \leq C  \\)

</section>

<section data-markdown>
# LAB SVC

[L11 N1 SVC.py]()

</section>

<section data-markdown>
# Support Vector Machine

What if the data is not even close to be linearly separable?

![](assets/11/non_linearly_separable.png)

Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)

</section>

<section data-markdown>
# Introducing Kernels

The optimization equation for the linear support vector classifier can be rewritten as such

$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i \langle x, x\_i \rangle   $$
$$ f(x) =  \beta\_0 + \sum\_{i=1}^{n} \alpha\_i K(x,x\_i)    $$

with \\( K(x,x\_i) = \langle x, x\_i \rangle \\) the vector dot product.

But we could use a different Kernel function


**Note**:This involves \\(p * \frac{n(n-1)}{2} \\) multiplications! However only the support vectors are useful. So we can limit the sum to \\(S\\) support vectors.


</section>

<section data-markdown>
# Kernels

Here are some classic Kernel functions

* Linear Kernel \\( \quad K(x,x_i) = \langle x, x_i \rangle\\)
* Polynomial Kernel (d): \\( \quad K(x,x\_i) =  (1 + \sum\_{j=1}^{p} x\_{j}x\_{i,j} )^d   \\)
* Radial Kernel \\( \quad K(x,x\_i) =   \exp(-\gamma    \sum\_{j=1}^{p} (x\_{j} - x\_{i,j})^2 )   \\)

</section>
<section data-markdown>
# SVM Lab

gads/11_svm_imbalanced/py/L11_N2_SVM.py
</section>
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Part II Imbalanced datasets</p>
</section>

<section data-markdown>
# Imbalanced datasets

Caravan dataset: a few hundreds YES vs thousands of NO

* Setting all predictions to No gives you a 94% prediction rate!

Strategies

* Collect more data
* Accuracy is not always the best metric (Cohen's Kappa, F1 score, ...)
* Oversample or Undersample
* SMOTE
* Decision trees often perform well on imbalanced datasets
* penalized-SVM

</section>

<section data-markdown>
# Lab use SVM on Caravan

In the next exercise we will use the Caravan dataset. Highly unbalanced.

* Visualize the set with 2 component PCA
* Split the set into train and  test (80/20)
* Grid search an SVM with C = [0.01, 0.1, 0.5, 1 ] and kernel = 'rbf'. Use the roc_auc as scoring
* Look at: the grid\_scores\_
* Predict the test set, and look at
    * The confusion matrix,
    * The classification report
    * The roc_auc

</section>

<section data-markdown>
# Lab use SVM on Caravan

## 1. dumb classifier
Create a simple classifier that always gives 'No' as the result.


Accuracy?

</section>

<section data-markdown>
# Lab

## 2. SVM

Grid search an SVM on the train set.

Accuracy?

Confusion Matrix on the test set?

Conclusion?

**Keep the initial X_test and y_test**

</section>

<section data-markdown>
# Undersampling
## 3. Under sample

In the initial dataset we only have 348 samples in the Yes class.

Build a dataframe with these 348 samples and a random selection of 348 No samples

Grid Search an SVM (same params as last step)

Score and confusion matrix on the original test set?

</section>

<section data-markdown>
# Over sampling
## 4. Over sample

Build a new dataframe by replicating the Yes set 4 times

Grid Search an SVM (same params)

Score and confusion matrix on the original test set?

</section>

<section data-markdown>
# SMOTE
## 5. Synthetic Minority Oversampling TEchnique

Install

> pip install -U imbalanced-learn


from imblearn.over_sampling import SMOTE
smox, smoy = smote.fit_sample(X_train, y_train)

and visualize with PCA

results?


</section>

