---
layout: slide
title: Lesson 11 - SVM
description: none
transition: slide
permalink: /11-stochastic-gradient-descent.html
theme: ga
---

# Part I: SVM

# Hyperplane

Classification separating an hyperplane

* 2D => line
* 3D => surface
* ...

* A hyperplane in 2 dimension is defined by the equation
\\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0  \\)


* In p dimension \\(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p = 0  \\)

# Context

We have a n samples in p dimensions (features) \\X = \{ x_{i}  i \in [1,n]  \}  \\).

These observations fall into two classes \\( y_i \in \{-1, +1 \} \\)

The classes are linearly separable: one side of the hyper plane corresponds to a class and the other side to the other class.

All points \\(x_i \\) such that  \\( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j}  > 0 \\)  belong to class 1
and All points  such that \\( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j}  < 0 \\) belong to class -1

We predict the class of a new point by calculating \\(f(x^∗) = \beta_0+ + \sum_{j=1}^{n} \beta_j x^*_{j}  \\)

* if f(x∗) >0 => 1
* if f(x∗) <0 => -1

A classifier that is based on a separating hyperplane leads to a **linear decision boundary**.

# One line

since \\( y_i\\) and \\( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j}  < 0 \\) have the same sign we always have

$$ y_i ( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j} ) > 0 $$

# Infinity of Hyperplanes

If our data is linearly separable then there exists an infinity of hyperplanes that can separate it

![](assets/11/hyperplane-classification.png)
Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)

=> Introducing the margin

# Maximal Margin Classifier

Find the Hyperplane that will maximise the distance to all the points. That will best separate the classes
![Maximal Margin Classifier](assets/11/Maximal_Margin_Classifier.png)

# Maximal Margin Classifier

$$ \max{\beta_i} M \text{such that} y_i ( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j} ) > M  \forall i \in [1,..,n] $$
with \\( \sum_{j=0}^{p}  \beta^2_j = 1\\)


Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)

# Support Vectors

The observations that are on the margins are called **support vectors**

They *support* the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyper- plane would move as well.

Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations: a movement to any of the other observations would not affect the separating hyperplane, provided that the observation’s movement does not cause it to cross the boundary set by the margin.

A change in the support vector impacts the margin a lot => over fitting

# Support Vector Classifier

When the classes are not separable, we still want the best margin possible

Some points will be on the wrong side

Even if linearly separable we want a classifier that does not perfectly separate the classes

* more robust

Add a tuning parameter C that dictates the severity of the violation of the margin

The bigger C is , the more points are

1. within the margin
2. misclassified

C is some budget for violation of the margin.

C is determined during cross validation

# Support Vector Classifier

\\( C\\) is a non negative tuning parameter

$$ \max{\beta_i, \epsilon_i} M \text{such that} y_i ( \beta_0 + \sum_{j=1}^{n} \beta_j x_{i,j} ) > M - \epsilon_i  \forall i \in [1,..,n]  $$
with \\( \sum_{j=0}^{p}  \beta^2_j = 1;  \epsilon_i > 0 \text{and} \sum_{i} \epsilon_1 \leq C  \\)

# LAB SVC

[L11 N1 SVC.py]()

# Support Vector Machine

What if the data is not linearly separable?

![](assets/11/non_linearly_separable.png)
Fig from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/getbook.html)

# Introducing Kernels

The optimization equation for the linear support vector classifier can be rewritten as such

$$ f(x) =  \beta_0 + \sum_{i=1}^{n} \alpha_i \langle x, x_i \rangle   $$
which involves \\(p * \frac{n(n-1)}{2} \\) multiplications!

But only the support vectors are useful. we can limit the sum to \\(S\\) support vectors

\\ K(x,x_i) = (\langle x, x_i \rangle\\) is the vector dot product.

We could use a different K function

# Kernels

Here are some classic Kernel functions

* Linear Kernel \\ K(x,x_i) = (\langle x, x_i \rangle\\)
* Polynomial Kernel (d) \\ K(x,x_i) =  (1 + \sum_{j=1}^{p} x_{i,j}x_{i^{\prime},j} )^d   \\)
* Radial Kernel \\ K(x,x_i) =   \exp(-\gamma    \sum_{j=1}^{p} (x_{i,j} - x_{i^{\prime},j})^2 )   \\)


# Part II Imbalanced datasets

# Imbalanced datasets

Caravan dataset

* a few hundreds yes vs many No
* some features: very few 1s lots of 0s

Imbalanced datasets can be problematic

* setting all predictions to No gives you a 95% prediction rate!

Strategies
* Collect more data
* Choose the right classfication Metric (Cohen's Kappa),  F1 score:a harmonic mean between precision and recall.
* Oversample
* Undersample
* SMOTE
* decision trees often perform well on imbalanced datasets
* penalized-SVM

# Lab use SVM on Caravan

In the next exercise we will use the Caravan dataset. Hightly unbalanced.

* visualize the set with 2 component PCA
* split the set into train and  test (80/20)
* Grid search an SVM with C = [0.01, 0.1, 0.5, 1 ] and kernel = 'rbf'. Use the roc_auc as scoring
* Look at: the grid_scores_
* Predict the test set, and look at
    * The confusion matrix,
    * The classisication report
    * The roc_auc

# Lab use SVM on Caravan

## 1. dumb classifier
Create a simple classifier that always gives 'No' as the result.

What is its accuracy?

# Lab

## 2. SVM

Grid search an SVM on the train set.
Accuracy? Confusion Matrix n the test set? Conclusion?

**Keep the initial X_test and y_test**

# Undersampling
## 3. Under sample

In the initial dataset we only have 348 samples in the Yes class.

Build a dataframe with these 348 samples and a random selection of 348 No samples

Grid Search an SVM (same params)

score and confusion matrix on the original test set?

# Over sampling
## 4. Over sample

Build a new dataframe by replicating the Yes set 4 times

Grid Search an SVM (same params)

score and confusion matrix on the original test set?

# SMOTE
## 5. Synthetic Minority Oversampling TEchnique

Install

> pip install -U imbalanced-learn


from imblearn.over_sampling import SMOTE
smox, smoy = smote.fit_sample(X_train, y_train)

and visualize with PCA

results?




