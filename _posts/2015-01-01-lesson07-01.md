---
layout: slide
title: Lesson 07 - Sampling, Bias - Variance and SGD
description: none
transition: slide
permalink: /07-sampling-bias-variance-sgd.html
theme: ga
---


<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>7: Sampling, Cross Validation, Bootstrapping, Bias Variance, Over/ Under fitting</p>
</section>

<section data-markdown>
# Lesson 7
## LEARNING OBJECTIVES

* Bias - Variance decomposition
* Cross validation
* Sampling, Boostrapping
* Strategies to deal with Overfitting and Underfitting

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #6</h1>
    <p class = 'big_title'>Review of Lesson 6</p>
</section>


<section data-markdown>
# Last Lesson Review
* Scikit: model, fit(), predict()
* Linear regression
* Loss function
* Ridge, Lasso, Elastic
* Polynomial regression
</section>

<section data-markdown>
# Last session
### Any questions from last class?

* What's a loss function?
* What is Ridge?
    * What's the impact of \\( \alpha \\)?
* What is Polynomial regression?

Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Making the most out of your data and Model selection</p>
</section>


<section data-markdown>
# Polynomial regression

if you remember we had the following plots for polynomial regression:

![polynomial regression](assets/07/polynomial_regression.png)

The higher the degree of the polynomial, the better the fit

How does it work with new data?
</section>

<!-- Over fitting / under fitting - Bias / Variance -->
<section data-markdown>
# Over fitting / under fitting

* Exercise: on housing dataset, with ridge (alpha)
* or on housing dataset with polynomial
* try to predict


[Notebook 1: Overfitting or Underfitting](http://localhost:8888/notebooks/07_bias_variance/L7%20Overfitting%20or%20Underfitting.ipynb)


</section>

<section data-markdown>
# Bias Variance decomposition

Error due to Bias: Error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis, thereby creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value.

Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.



</section>

<section data-markdown>
# Bias Variance decomposition
![Bias-variance](assets/07/Bias-variance-targets.png)
</section>

<section data-markdown>
# Bias Variance decomposition

<!-- The Mean Squared Error  is the mean ( \frac {1}{n}}\sum _{i=1}^{n}} {\frac {1}{n}}\sum _{{i=1}}^{n}) of the square of the errors ( ( Y i ^ âˆ’ Y i ) 2 {\displaystyle ({\hat {Y_{i}}}-Y_{i})^{2}} ({\hat {Y_{i}}}-Y_{i})^{2})
 -->
$$ \operatorname{MSE}( \hat{Y} ) = \frac {1}{n} \sum_{i=1}^{n}( \hat{Y_i}-Y_i )^2  $$

Which can be rewritten as
$$ \operatorname{MSE}( \hat{Y} )= \mathbb{E} \big[ (\hat{Y} - Y)^2   \big]  =\mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2\right] + \left(\mathbb{E}(\hat{Y})-Y\right)^2 $$

$$ \operatorname{MSE}( \hat{Y} )= \operatorname{Var}(\hat{Y})+ \operatorname{Bias}(\hat{Y},Y)^2 $$

with

* \\( \operatorname{Var}(\hat{Y}) =  \mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2\right] \\)
* \\( \operatorname{Bias}(\hat{Y},Y)  = \mathbb{E}(\hat{Y})-Y  \\)
</section>

<section>
$$
\begin{align} \mathbb{E}((\hat{Y}-Y)^2)&=
 \mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})+\mathbb{E}(\hat{Y})-Y\right)^2\right]
\\ & =
\mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2 +2\left((\hat{Y}-\mathbb{E}(\hat{Y}))(\mathbb{E}(\hat{Y})-Y)\right)+\left( \mathbb{E}(\hat{Y})-Y \right)^2\right]
\\ & = \mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2\right]+2\mathbb{E}\Big[(\hat{Y}-\mathbb{E}(\hat{Y}))(\overbrace{\mathbb{E}(\hat{Y})-Y}^{\begin{smallmatrix} \text{This is} \\  \text{a constant,} \\ \text{so it can be} \\  \text{pulled out.} \end{smallmatrix}}) \,\Big] + \mathbb{E}\Big[\,\overbrace{\left(\mathbb{E}(\hat{Y})-Y\right)^2}^{\begin{smallmatrix} \text{This is a} \\  \text{constant, so its} \\  \text{expected value} \\  \text{is itself.} \end{smallmatrix}}\,\Big]
\\ & = \mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2\right]+2\underbrace{\mathbb{E}(\hat{Y}-\mathbb{E}(\hat{Y}))}_{=\mathbb{E}(\hat{Y})-\mathbb{E}(\hat{Y})=0}(\mathbb{E}(\hat{Y})-Y)+\left(\mathbb{E}(\hat{Y})-Y\right)^2
\\ & = \mathbb{E}\left[\left(\hat{Y}-\mathbb{E}(\hat{Y})\right)^2\right]+\left(\mathbb{E}(\hat{Y})-Y\right)^2
\\ & = \operatorname{Var}(\hat{Y})+ \operatorname{Bias}(\hat{Y},Y)^2
\end{align}

$$
</section>
<section data-markdown>
# Recap

We want to reduce the Mean Squared Error

* We have to reduce both the Bias and the Variance
* High Bias
    * Underfitting
    * Not a good estimator
* High Variance
    * Overfitting
    * too **sensitive** to training data
    * no predictive power

</section>



<!-- cross validation -->
<section  data-background-color="#22c8c6">
    <h1>Cross Validation</h1>
    <p class = 'big_title'>Is your model good? is it predictive?</p>
</section>
<section data-markdown>
# meta parameters - tuning your model

* \\(\alpha \\) in Ridge
        class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)

* Degree in polynomial regression
        class sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)

* K in K-NN
        class sklearn.neighbors.NearestNeighbors(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1, **kwargs)

Meta parameters allow you to train several different models on a given set of data
</section>

<section data-markdown>
# Train, validation, test

Split your data in 3 subsets

* **Training set** (60% of the original data set): This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.

* **Cross-Validation** set (20% of the original data set): This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.

* **Test set** (20% of the original data set): Now we have chosen our preferred prediction algorithm but we don't know yet how it's going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it's going to perform so we can have an idea about our algorithm's performance on unseen data.

</section>

<section data-markdown>
# Train, validation, test
### Example

* Split your data in 3 subsets: train validation and test
* Train 10 Ridge models with 10 different \\(\alpha \\)
* Calculate MSE on the validation set for these 10 models
* Pick the best one
* Run the best one on the test set and calculate the resulting MSE
* **Do not touch your model anymore**

</section>

<section data-markdown>
# Train, validation, test
* Exercise: Ridge on housing with different values of \\( \alpha \\)

[Notebook - train validation test split](http://localhost:8888/notebooks/07_bias_variance/py/L7%20train%20validation%20test.ipynb)

</section>

<section data-markdown>
# K-fold cross validation

Train, valid, test: You're wasting a lot of data

### [K-fold cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)

* split your data into train / test, leave the test alone

Then

* further split your training set on K subsets (K = 4)
* train on 1,2,3, validate on 4
* train on 1,2,4, validate on 3
* train on 1,3,4, validate on 2
* train on 2,3,4, validate on 1

Mean error on validation is a better estimation on the performance of your model than if you had just split 3 subsets.

</section>

<section data-markdown>
# K-fold cross validation
![K-fold cross validation](assets/07/k-fold-cross-validation.png)

Many other ways to do [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation) in Scikit learn

</section>

<section data-markdown>
# K-fold cross validation
Exercise

On the diabetes dataset, find the optimal regularization parameter alpha.

Bonus: How much can you trust the selection of alpha?

http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold

http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py

</section>

<!-- Bootstraping -->
<section  data-background-color="#22c8c6">
    <h1>Bootstraping</h1>
    <p class = 'big_title'>More data out of your data</p>
    <p class = 'big_title'>What if you have a small dataset?</p>
</section>

<section data-markdown>
# Bootstrapping
You have a small number of samples (N samples).
You want a good understanding of some statictics / measure of the overall population: for instance the mean, or the Standard Deviation

1. You calculate the mean on your whole sample

Gives you one number but not much in terms of the reliability of this number.

2. You run N experiments
    * Each time you select half of your samples with replacement
        Which means some samples are counted selevarl times
    * You calculate the mean for each experiment
You end up with a pretty solid estimation of the distribution of the mean of your population.

[Bootstrapping animated](https://www.stat.auckland.ac.nz/~wild/BootAnim/)

</section>

<section data-markdown>
# Bootstrapping

[Notebook - Boostrapping for the mean](http://localhost:8888/notebooks/07_bias_variance/py/L7%20Boostrapping%20the%20mean.ipynb#)

</section>

<section data-markdown>
# Bootstrapping for model selection
http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.cross_validation.Bootstrap.html
http://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html

exercise: estimate the distribution of your linear regression coeffficients
</section>

<!-- Learning Curve -->
<section  data-background-color="#22c8c6">
    <h1>Learning Curve</h1>
    <p class = 'big_title'>Is your model overfitting or under fitting</p>
</section>

<section data-markdown>
# Over fitting or Under fitting?

Split your data into 2 sets: training and testing
http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py

http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html#sklearn.learning_curve

</section>

<section data-markdown>
# Over fitting or Under fitting?
Exercise: polynomial with n = 3 on housing dataset
plot the learning curve

estimator = ...
X_train, y_train, X_test, y_test = split(X, y)
n_samples = X_train.shape[0]
train_scores, test_scores = [], []
for n in range(10, 10, n_samples):
    estimator.fit(X_train[:n], y_train[n])
    train_scores.append(estimator.score(X_train[:n], y_train[n]))
    test_scores.append(estimator.score(X_test, y_test))
plot(range(10, 10, n_samples), train_scores)
plot(range(10, 10, n_samples), test_scores)

</section>
<!-- strategies -->
<section  data-background-color="#22c8c6">
    <h1>Strategies</h1>
    <p class = 'big_title'>Strategies</p>
</section>

<section data-markdown>
# High Bias - Under fitting
http://www.astroml.org/sklearn_tutorial/practical.html

* Add more features. In our example of predicting home prices, it may be helpful to make use of information such as the neighborhood the house is in, the year the house was built, the size of the lot, etc. Adding these features to the training and test sets can improve a high-bias estimator
* Use a more sophisticated model. Adding complexity to the model can help improve on bias. For a polynomial fit, this can be accomplished by increasing the degree d. Each learning technique has its own methods of adding complexity.

* Decrease regularization. Regularization is a technique used to impose simplicity in some machine learning models, by adding a penalty term that depends on the characteristics of the parameters. If a model has high bias, decreasing the effect of regularization can lead to better results.

</section>

<section data-markdown>
# High Variance - Over fitting

* Use fewer features. Using a feature selection technique may be useful, and decrease the over-fitting of the estimator.
* Use more training samples. Adding training samples can reduce the effect of over-fitting, and lead to improvements in a high variance estimator.
* Increase Regularization. Regularization is designed to prevent over-fitting. In a high-variance model, increasing regularization can lead to better results.
* Bagging to reduce over fitting (average several overfitting models)
https://class.coursera.org/datasci-001/lecture/157
https://en.wikipedia.org/wiki/Bootstrap_aggregating

</section>

<!--
* Context
In this lesson we will focus on managing our data to assess, preserve and boost the predictive power of our model

* Bias Variance decomposition
    * Explanation
    * Math

* How to test the predictability potential of your model?
    * Split train and validation / test
    * Exercise: linear model prediction on ozone?
    * Cross validation
    * Exercise on housing.data.csv

* Sampling for cross validation
    * Bootstraping: random sampling with replacement
    https://www.stat.auckland.ac.nz/~wild/BootAnim/
    https://class.coursera.org/datasci-001/lecture/155
    * Exercise: Estimate the mean see https://en.wikipedia.org/wiki/Bootstrapping_(statistics)
    * Boostrap to estimate confidence interval of your results
        * exercise: estimate the distribution of your linear regression coeffficients

    * Leave one out

* Learning curve?

* Regularization to reduce over fitting

* Bagging to reduce over fitting
https://class.coursera.org/datasci-001/lecture/157
https://en.wikipedia.org/wiki/Bootstrap_aggregating

* Boosting:  favor the misclassified points

* Stochastic Gradient Descent
 -->

<section data-markdown>
# links

[Underfitting vs. Overfitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#example-model-selection-plot-underfitting-overfitting-py)
</section>