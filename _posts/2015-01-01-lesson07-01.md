---
layout: slide
title: Lesson 07 - Sampling, Bias - Variance and SGD
description: none
transition: slide
permalink: /07-sampling-bias-variance-sgd.html
theme: ga
---


<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>Sampling, Cross Validation, Bias Variance, SGD</p>
</section>

<section data-markdown>
# Title
## LEARNING OBJECTIVES

* Bias - Variance decomposition
* Cross validation
* Sampling
* Strategies to deal with  Over fitting and Under fitting

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #6</h1>
    <p class = 'big_title'>Review of Lesson 6</p>
</section>


<section data-markdown>
# Last Lesson Review
* Scikit
* Linear regression
* Ridge, Lasso, elastic
* Polynomial regression

</section>

<section data-markdown>
# Last session
### Any questions from last class?

* What is Ridge?
    * What's the impact of \\( \alpha \\)?
* What is Polynomial regression?

Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Making the most out of your data</p>
    <p class = 'big_title'>Model selection</p>
</section>


<section data-markdown>
# Polynomial regression

if you remember we had the following plots:


n=1 Not great
n=2 good
n=16 too much

http://www.astroml.org/sklearn_tutorial/practical.html

How does it work with new data?
</section>

<!-- Over fitting / under fitting - Bias / Variance -->
<section data-markdown>
# Over fitting / under fitting

* Exercise: on housing dataset, with ridge (alpha)
* or on housing dataset with polynomial
* try to predict


Notebook 1:
see http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#example-model-selection-plot-underfitting-overfitting-py
to generate data
</section>

<section data-markdown>
# Bias Variance decomposition
Feeling it
</section>

<section data-markdown>
# Bias Variance decomposition
Math
</section>

<!-- cross validation -->
<section  data-background-color="#22c8c6">
    <h1>cross validation</h1>
    <p class = 'big_title'>Is your model good? is it predictive?</p>
</section>
<section data-markdown>
# meta parameters - tuning your model

* alpha in ridge
* degree in polynomial regression
* K in K-nearest-neighbors
....

</section>

<section data-markdown>
# Train, validation, test

Split your data in 3 subsets

* Training set: train multiple algorithms with different parameters
* Validation set: Assess score of each of these models on validation set

=> choose best model that minimizes error on training set and validation set

=> test set to see the real predictive power of your model

* Training set (60% of the original data set): This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.

* Cross-Validation set (20% of the original data set): This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.

* Test set (20% of the original data set): Now we have chosen our preferred prediction algorithm but we don't know yet how it's going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it's going to perform so we can have an idea about our algorithm's performance on unseen data.

</section>

<section data-markdown>
# Train, validation, test
* Exercise: Ridge on housing with alpha = [0.01, 0.1, 0.5, 1, 2]?

http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split
</section>

<section data-markdown>
# K-fold validation

You're wasting a lot of data

* split your data into train / test, leave the test alone

* further split your training set on K subsets (K = 4)

* train on 1,2,3, validate on 4
* train on 1,2,4, validate on 3
* train on 1,3,4, validate on 2
* train on 2,3,4, validate on 1

Mean error on validation is a better estimation on the performance of your model than if you had just split 3 subsets.

</section>

<section data-markdown>
# K-fold validation
* Exercise: K-fold validation in scikit. Ridge on housing with alpha = [0.01, 0.1, 0.5, 1, 2]?
http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold
</section>

<!-- Bootstraping -->
<section  data-background-color="#22c8c6">
    <h1>Bootstraping</h1>
    <p class = 'big_title'>More data our of your data</p>
    <p class = 'big_title'>What if you have a small dataset?</p>
</section>

<section data-markdown>
# Bootstrapping
Sample with replacement

Some stat (Mean , std, ...) of a population

    https://www.stat.auckland.ac.nz/~wild/BootAnim/
    https://class.coursera.org/datasci-001/lecture/155

</section>

<section data-markdown>
# Bootstrapping
estimate confidence intervals

</section>

<section data-markdown>
# Bootstrapping for model selection
http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.cross_validation.Bootstrap.html
http://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html

exercise: estimate the distribution of your linear regression coeffficients
</section>

<!-- Learning Curve -->
<section  data-background-color="#22c8c6">
    <h1>Learning Curve</h1>
    <p class = 'big_title'>Is your model overfitting or under fitting</p>
</section>

<section data-markdown>
# Over fitting or Under fitting?

Split your data into 2 sets: training and testing
http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-selection-plot-learning-curve-py
http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html#sklearn.learning_curve
</section>

<section data-markdown>
# Over fitting or Under fitting?
Exercise: polynomial with n = 3 on housing dataset
plot the learning curve

estimator = ...
X_train, y_train, X_test, y_test = split(X, y)
n_samples = X_train.shape[0]
train_scores, test_scores = [], []
for n in range(10, 10, n_samples):
    estimator.fit(X_train[:n], y_train[n])
    train_scores.append(estimator.score(X_train[:n], y_train[n]))
    test_scores.append(estimator.score(X_test, y_test))
plot(range(10, 10, n_samples), train_scores)
plot(range(10, 10, n_samples), test_scores)

</section>
<!-- strategies -->
<section  data-background-color="#22c8c6">
    <h1>Strategies</h1>
    <p class = 'big_title'>Strategies</p>
</section>

<section data-markdown>
# High Bias - Under fitting
http://www.astroml.org/sklearn_tutorial/practical.html

* Add more features. In our example of predicting home prices, it may be helpful to make use of information such as the neighborhood the house is in, the year the house was built, the size of the lot, etc. Adding these features to the training and test sets can improve a high-bias estimator
* Use a more sophisticated model. Adding complexity to the model can help improve on bias. For a polynomial fit, this can be accomplished by increasing the degree d. Each learning technique has its own methods of adding complexity.

* Decrease regularization. Regularization is a technique used to impose simplicity in some machine learning models, by adding a penalty term that depends on the characteristics of the parameters. If a model has high bias, decreasing the effect of regularization can lead to better results.

</section>

<section data-markdown>
# High Variance - Over fitting

* Use fewer features. Using a feature selection technique may be useful, and decrease the over-fitting of the estimator.
* Use more training samples. Adding training samples can reduce the effect of over-fitting, and lead to improvements in a high variance estimator.
* Increase Regularization. Regularization is designed to prevent over-fitting. In a high-variance model, increasing regularization can lead to better results.
* Bagging to reduce over fitting (average several overfitting models)
https://class.coursera.org/datasci-001/lecture/157
https://en.wikipedia.org/wiki/Bootstrap_aggregating

</section>

<!--
* Context
In this lesson we will focus on managing our data to assess, preserve and boost the predictive power of our model

* Bias Variance decomposition
    * Explanation
    * Math

* How to test the predictability potential of your model?
    * Split train and validation / test
    * Exercise: linear model prediction on ozone?
    * Cross validation
    * Exercise on housing.data.csv

* Sampling for cross validation
    * Bootstraping: random sampling with replacement
    https://www.stat.auckland.ac.nz/~wild/BootAnim/
    https://class.coursera.org/datasci-001/lecture/155
    * Exercise: Estimate the mean see https://en.wikipedia.org/wiki/Bootstrapping_(statistics)
    * Boostrap to estimate confidence interval of your results
        * exercise: estimate the distribution of your linear regression coeffficients

    * Leave one out

* Learning curve?

* Regularization to reduce over fitting

* Bagging to reduce over fitting
https://class.coursera.org/datasci-001/lecture/157
https://en.wikipedia.org/wiki/Bootstrap_aggregating

* Boosting:  favor the misclassified points

* Stochastic Gradient Descent
 -->
