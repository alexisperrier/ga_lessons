---
layout: slide
title: Lesson 08 - Classification - K-Nearest Neighbor
description: none
transition: slide
permalink: /08-classification-knn.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>8. Classification, KNN, Curse of dimensionality, Iris, Digits, Wine datasets  </p>
</section>

<section data-markdown>
# Lesson 7
## LEARNING OBJECTIVES

* Classification
* KNN
* Curse of dimensionality
* Optimizing meta parameters with Grid search
* Visualization of higher dimensions

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #8</h1>
    <p class = 'big_title'>Review of Lesson 7</p>
</section>


<section data-markdown>
# Last Lesson Review

* Bias - Variance decomposition
* Cross validation
* Sampling, Boostrapping
* Strategies to deal with Overfitting and Underfitting

</section>

<section data-markdown>
# Last session
### Any Questions?

* How to reduce Bias / Underfitting?
* How to reduce Variance / Overfitting?

Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Classification</p>
</section>


<section data-markdown>
# Classification

### Regression
*  Predict Continuous values

### Classification
Predict a finite set of values aka labels

* Binary classification (sex of the baby)
* Multiclass classification ()

</section>


<section data-markdown>
# Regression or Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

=> Using Threshold(s)

        if probability < 0.5:
            return A
        else:
            return B

</section>

<section data-markdown>
#  Build a classifier!

# Your turn  N1

* Load the iris dataset
* Boxplot the ... vs the class

Write a simple python function that returns 0, 1 depending on the Sepal ...
and a threshold

Try a few different thresholds

For each threshold calculate the number of estimated positives (1s) over the true number of samples in that class

What happens to the accuracy if you were to look at ... for your simple classifier? Try it.

see: https://github.com/alexperrier/ds-curriculum/blob/master/lessons/lesson-08/code/starter-code/starter-code-8.ipynb
</section>

<section data-markdown>
# K Nearest Neighbors

You have a set of samples already labeled

1. For each new sample, calculate the *distance* to **all** other points.
2. Given those distances, pick the **K** closest samplessamples.
2. Count the number of samples in each class
4. The new sample is classified as the class label with the largest number ("votes").

</section>

<section data-markdown>
# K Nearest Neighbors
Iris dataset
Demo: KNN In Action

</section>

<section data-markdown>
# Your turn  N2

Pseudo code

* set the seed
* Load iris dataset and shuffle it
* split X and y in train / test dataset (100, 50 samples)
* (loop over K) for each K
    * Initialize the classifier with K
    * Fit
    * Predict
    * print K and the accuracy score


Iris
Manual find * Find K as a function of Accuracy

</section>

<section data-markdown>
# Grid search

* Now Find K with Grid search

</section>

<section data-markdown>
# Distance

* What about manhattan_distance, minkowski_distance
</section>

<section data-markdown>
# Distance
* Scaling and centering

https://www.datacamp.com/community/tutorials/the-importance-of-preprocessing-in-data-science-and-the-machine-learning-pipeline-i-centering-scaling-and-k-nearest-neighbours


use ? http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification
with n_features from 2 to 100 ...

</section>

<section data-markdown>
# Distance

* Visualizing multi dimension dataset
http://bigdata-madesimple.com/k-nearest-neighbors-curse-dimensionality-python-scikit-learn/

</section>

<section data-markdown>
# Distance
* Curse of dimensionality!!!
http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/
</section>

<section data-markdown>
# Other classifiers

* Try other classifiers on Digits: Logistic, SVM,
http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py

See code on http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html#example-exercises-digits-classification-exercise-py
</section>


<section data-markdown>
# Other classifiers
* KNN for regression

http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html
</section>


<section data-markdown>
# Links
When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

https://blog.cambridgecoding.com/2016/01/16/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm/

https://www.datacamp.com/community/tutorials/the-importance-of-preprocessing-in-data-science-and-the-machine-learning-pipeline-i-centering-scaling-and-k-nearest-neighbours

http://bigdata-madesimple.com/k-nearest-neighbors-curse-dimensionality-python-scikit-learn/

https://www.youtube.com/watch?v=4ObVzTuFivY&index=6&list=PLD0F06AA0D2E8FFBA

http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric

https://aidaiict.wordpress.com/2013/10/24/building-a-hand-written-digit-identifier-and-some-machine-learning-basics/

http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py

http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors

</section>