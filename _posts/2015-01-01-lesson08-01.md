---
layout: slide
title: Lesson 08 - Classification - K-Nearest Neighbor
description: none
transition: slide
permalink: /08-classification-knn.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>8. Classification, KNN, Curse of dimensionality, Iris, Digits, Wine datasets  </p>
</section>

<section data-markdown>
# Lesson 7
## LEARNING OBJECTIVES

* Classification
* KNN
* Curse of dimensionality
* Optimizing meta parameters with Grid search
* Visualization of higher dimensions

</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #8</h1>
    <p class = 'big_title'>Review of Lesson 7</p>
</section>


<section data-markdown>
# Last Lesson Review

* Bias - Variance decomposition
* Cross validation
* Sampling, Boostrapping
* Strategies to deal with Overfitting and Underfitting

</section>

<section data-markdown>
# Last session
### Any Questions?

* How to reduce Bias / Underfitting?
* How to reduce Variance / Overfitting?

Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Classification</p>
</section>


<section data-markdown>
# Classification

### Regression
*  Predict Continuous values

### Classification
Predict a finite set of values aka labels

* Binary classification (sex of the baby)
* Multiclass classification ()

</section>


<section data-markdown>
# Regression or Classification?
Review the following situations and decide if each one is a regression problem, classification problem, or neither:

* Using the total number of explosions in a movie, predict if the movie is by JJ Abrams or Michael Bay.
* Determine how many tickets will be sold to a concert given who is performing, where, and the date and time.
* Given the temperature over the last year by day, predict tomorrow's temperature outside.
* Using data from four cell phone microphones, reduce the noisy sounds so the voice is crystal clear to the receiving phone.
* With customer data, determine if a user will return or not in the next 7 days to an e-commerce website.

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

</section>

<section data-markdown>
# Regression to Classification

How can regression models be used for classification?

=> Using Threshold(s)

        if probability < 0.5:
            return A
        else:
            return B

</section>

<section data-markdown>
#  Build a classifier!

# Your turn  N1

* Load the iris dataset
* Boxplot the ... vs the class

Write a simple python function that returns 0, 1 depending on the Sepal ...
and a threshold

Try a few different thresholds

For each threshold calculate the number of estimated positives (1s) over the true number of samples in that class

What happens to the accuracy if you were to look at ... for your simple classifier? Try it.

see: https://github.com/alexperrier/ds-curriculum/blob/master/lessons/lesson-08/code/starter-code/starter-code-8.ipynb
</section>

<section data-markdown>
# K Nearest Neighbors

You have a set of samples already labeled

1. For each new sample, calculate the *distance* to **all** other points.
2. Given those distances, pick the **K** closest samplessamples.
2. Count the number of samples in each class
4. The new sample is classified as the class label with the largest number ("votes").

</section>

<section data-markdown>
# K Nearest Neighbors
Iris dataset
Demo: KNN In Action

</section>

<section data-markdown>
# Your turn  N2

Pseudo code

* set the seed
* Load iris dataset and shuffle it
* split X and y in train / test dataset (100, 50 samples)
* (loop over K) for each K
    * Initialize the classifier with K
    * Fit
    * Predict
    * print K and the accuracy score


Iris
Manual find * Find K as a function of Accuracy

</section>

<section data-markdown>
# Grid search: Searching for estimator parameters

How do you find the best hyperparameter for your model?

* \\(\alpha\\) for Ridge or Lasso
* K for K-NN
* ...

[Grid search](http://scikit-learn.org/stable/modules/grid_search.html) and [Randomized Search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV) allow you to specify a parameter space and find which value(s) of the hyperparameter(s) results in the best performance of your model.

</section>
<section data-markdown>
# Grid search: Searching for estimator parameters

### Metric
* Accuracy for classification
* \\(R^2\\) for regression
* Specify another metric via the *scoring* function

</section>

<section data-markdown>
# Grid search: Your turn

Now Find K with Grid Search on Fraudulent dataset

* fraud: cases
* student: is the person a student
* balance and income

1. walkthrough
    * knn = neighbors.KNeighborsClassifier()
    * parameters = {'n_neighbors' : np.arange(2,20) }
    * clf = GridSearchCV(knn, parameters)
    * clf.best_estimator_,  clf.best_params_, clf.best_score_
2. just as function of balance
    * looking at the accuracy, what's the best K?
    * what's happening here?

</section>

<section data-markdown>
# Minkowski Distance

for 2 points  \\(   X=(x_1,x_2,\ldots,x_n)\text{ and }Y=(y_1,y_2,\ldots,y_n) \in \mathbb{R}^n\\)

The Minkowski distance is defined by $$ d(X,Y) = \left(\sum_{i=1}^n |x_i-y_i|^p\right)^{1/p}  \text{ with } p \geq 1 $$

* For p = 1:[Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry)
$$   d(X,Y) = \sum_{i=1}^n |x_i-y_i| $$

* For p = 2: [Euclidian distance](https://en.wikipedia.org/wiki/Euclidean_distance)
$$ d(X,Y) =  \sqrt{\sum_{i=1}^n (x_i-y_i)^2}. $$

</section>
<section data-markdown>
# Grid search with K and distance

* parameters = {'n_neighbors' : np.arange(2,20), 'p': np.linspace(1,2,5) }
* What's the best distance and K?

</section>

<section data-markdown>
# Pre processing the data with scikit


[preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html)

* scaling
* normalizing
* remove linear correlation with PCA
* binarization
* encoding categorical features
* missing values

with

* fit
* transform

</section>

<section data-markdown>
# Scaling and centering

Let's scale and normalize the income and balance and see if that helps




</section>

<section data-markdown>
# Make normalization

use ? http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification
with n_features from 2 to 100 ...

n_features = n_informative + n_redundant + n_repeated + noisy feature

* influence of n_redundant, noisy features,
* does repeating features help?

</section>


<section data-markdown>
# Dimensionality reduction with PCA
### Principal Component Analysis

Problem: Can't visualize n Dimensions

Goal: Project the data space to a 2 dimension space to visualize it

PCA: orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components

* Visualizing multi dimension dataset
http://bigdata-madesimple.com/k-nearest-neighbors-curse-dimensionality-python-scikit-learn/

</section>

<section data-markdown>
# Curse of dimensionality

* Curse of dimensionality!!!
http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/

</section>

<section data-markdown>
# Curse of dimensionality

Use make classification to demonstrate the curse of dimensionality

when n_features gets bigger comapred to samples.
</section>

<section data-markdown>
# Other classifiers

* Try other classifiers on Digits: Logistic, SVM,
http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py

See code on http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html#example-exercises-digits-classification-exercise-py
</section>


<section data-markdown>
# Other classifiers
* KNN for regression

http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html
</section>


<section data-markdown>
# Links
When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

https://blog.cambridgecoding.com/2016/01/16/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm/

https://www.datacamp.com/community/tutorials/the-importance-of-preprocessing-in-data-science-and-the-machine-learning-pipeline-i-centering-scaling-and-k-nearest-neighbours

http://bigdata-madesimple.com/k-nearest-neighbors-curse-dimensionality-python-scikit-learn/

https://www.youtube.com/watch?v=4ObVzTuFivY&index=6&list=PLD0F06AA0D2E8FFBA

http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric

https://aidaiict.wordpress.com/2013/10/24/building-a-hand-written-digit-identifier-and-some-machine-learning-basics/

http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py

http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors

http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors



http://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/classification_in_python.ipynb


</section>