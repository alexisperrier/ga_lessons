---
layout: slide
title: Lesson 12 - Decision Trees - Random forests
description: none
transition: slide
permalink: /12-decision-trees-random-forests.html
theme: ga
---

# Decision Trees

Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.

Rule based models

        if x < a
            if y < c
                ...
            else
                ...
        elif x > a & a < b
            ...
        else
            if z < d
                ...
            else
                0

# Decision Trees


* Simple to understand and to interpret. Trees can be visualised.
* Requires little data preparation. (missing values, scaling, dummy variables, ...)
* Low  cost for prediction \\( O(log(n)) \\) with n number of data points used to train the tree.
* Can handle both numerical and categorical data.
* Uses a white box model. An observed situation can simply be explained by boolean logic.
* Possible to validate a model using statistical tests.
* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.

# Decision Trees


* high overfitting for over-complex trees that do not generalise the data well.
* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated.
* no globally optimal decision tree

# Iris dataset

![](assets/12/L12-tree-iris.png)

# Controlling the tree

Regression over the diabetes dataset:

set these params to control the tree complexity

* max_depth
* min_samples_split
* min_samples_leaf
* max_features

http://localhost:8889/notebooks/12_decision_trees/py/L12%20N1%20Decision%20trees.ipynb

# Lab

        diabetes = load_diabetes()
        X = diabetes.data
        y = diabetes.target

        clf = DecisionTreeRegressor(max_depth = 10, min_samples_split = 10)
        scores = cross_val_score(clf, X, y, cv=5)
        print("scores: %0.2f"% np.mean(scores))

# Classification vs Regression

* MSE
* Gini


# Bootstrap aggregation aka Bagging

We used Boostrapping to estimate the mean of a sample.

Sampling with replacement.

1 sample becomes N samples.

The idea is the same with trees or any other classifier.

# Bagging for trees

* Generate B different bootstrapped training data sets.
* Train a new tree on each training set

The predictions of all the trees are averaged

=> significantly reduces over fitting

# Out Of Bag - OOB

When boostrapping, in each experiment will use only approx. 2/3rd of the available samples.

Which leaves 1/3rd that we can use to estimate the validation error of each tree.

This is called OOB Out of Bag error.

It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error.

# Feature inmportance

When the max_features < total number of features. Some features are left out of the splitting decision in each node.

Relative Feature importance can be deduced from the delta in MSE associated to the features included vs left out.

# Bagging in scikit

Use the [Bagging Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)

A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.

# Bagging in scikit
Very flexible

        bagging = BaggingClassifier(DecisionTreeClassifier(),  boostrap = True)


class sklearn.ensemble.BaggingClassifier

* base_estimator: The base estimator, decision tree by default
* n_estimators: How many estimators will be ensembled
* max_samples: The number of samples to draw from X to train each base estimator
* max_features: The number of features to draw from X to train each base estimator
* bootstrap: True / False
* bootstrap_features: True / False
* oob_score: True / False

# Bagging in scikit

Lab

# Random Forests

* In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features.

As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.

# Random Forest Lab


# Boosting

GBM


# Adaboost



