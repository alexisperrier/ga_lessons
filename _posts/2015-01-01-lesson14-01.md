---
layout: slide
title: Lesson 14 - NLP - Latent Models
description: none
transition: slide
permalink: /14-nlp-latent-models.html
theme: ga
---

# Last lesson review

* Bag of Words
* Pipeline in scikit
* Hashing trick


# Latent variable models

Attempting to uncover structure or organization inherent in the text.
Not supervised classification

unsupervised learning techniques

# Application

Topic Modeling

These techniques are commonly used for recommending news articles or mining large troves of data data and trying to find commonalities. Topic modeling, a method we will discuss in today's class, is used in the NY times recommendation engine. They attempt to map their articles to a latent space (or underlying structure) of topics using the content of the article.

# Goal of Topic Modeling
Fast and easy birds eye view of the large datasets. What is the data about?
What are the key themes?
Very powerful when coupled with different covariates (e.g., year
of publication or author):
Longitudinal analysis: How the key themes change over time?
Focus of discussion: Who is focussing on one topic, who talks about variety of top

# Goal of Topic Modeling
 ![](assets/14/topic_modeling_goal.png)

* Documents are about several topics at the same time. Topics are associated with different words.
* Topics in the documents are expressed through the words that are use

# Mixture Model
 ![](assets/14/lda-mixture-graphic.jpg)

# Example

[Topic Modeling in historical Newspapers](http://dl.acm.org/citation.cfm?id=2107649)
[Topic Modeling for the Social Sciences](http://vis.stanford.edu/files/2009-TopicModels-NIPS-Workshop.pdf)

# Techniques

Vector-based techniques:

* Latent Semantic Analysis (LSA) (a.k.a Latent Semantic Indexing - LSI)

Probabilistic techniques

* Probabilistic Latent Semantic Analysis (pLSA)
* Latent Dirichlet Allocation (LDA)
    * Whole bunch of LDA extensions

# Latent Semantic Analysis (LSA)

This is our corpus

* D1: modem the steering linux. modem, linux the modem. steering the modem. linux!
* D2: linux; the linux. the linux modem linux. the modem, clutch the modem. petrol.
* D3: petrol! clutch the steering, steering, linux. the steering clutch petrol. clutch the petrol; the clutch.
* D4: the the the. clutch clutch clutch! steering petrol; steering petrol petrol; steering petrol!!!!

Preprocessed

* D1: modem the steering linux modem linux the modem steering the modem linux
* D2: linux the linux the linux modem linux the modem clutch the modem petrol
* D3: petrol clutch the steering steering linux the steering clutch petrol clutch the petrol the clutch
* D4: the the the clutch clutch clutch steering petrol steering petrol petrol steering petrol

# Document Term Matrix

![](assets/14/document_term_matrix.png)

This matrix can be huge

How can we reduce it and at the same time uncover the topics?

# Latent Semantic Analysis

Nothing more than a singular value decomposition (SVD) of document-term matrix:

Find three matrices U, Σ and V so that: X = UΣVt

![](assets/14/lsa_decomposition.png)

# Latent Semantic Analysis
For example with 5 topics, 1000 documents and 1000 word vocabulary: Original matrix: 1000 x 1000 = 106
LSA representation: 5x1000 + 5 + 5x1000 ~ 10^4
-> 100 times less space

# Latent Semantic Analysis

![](assets/14/lsa_decomposition_example_01.png)

# Latent Semantic Analysis

![](assets/14/lsa_decomposition_example_02.png)

# Latent Semantic Analysis

![](assets/14/lsa_decomposition_example_03.png)


# Lab LSA

# Probabilistic LSA

What is a topic?

A list of probabilities for each of the possible words in a vocabulary.

Example topic:

● dog: 5%
● cat: 5%
● hause: 3%
● hamster: 2%
● turtle: 1%
● calculus: 0.000001%
● analytics: 0.000001%


# Probabilistic LSA

![](assets/14/probabilistic_topic.png)

# Probabilistic LSA

Instead of finding lower-ranked matrix representation, we can try to find a mixture of
word->topic & topic->documents distributions that are most likely given the observed documents.

* We define a statistical model of how the documents are being made (generated).
    * This is called generative process in topic modeling terminology.
* Then we try to find parameters of that model that best fit the observed data

# Generative Process

We just received a 50 word long document by our news reporter John Doe.
He is allowed to write only about one of the 6 possible topics, using only 6 words that exist in the world.
This is how we imagine that he wrote that article:
● For the first word, he throws a dice that tells him what is the topic of the first word. Say it is topic 1 (IT)
● Then he throws another dice to pick which word to use to describe topic 1. Say it is word 1 (linux)
● The process is repeated for all 50 words in the document.
● One thing! Dices are weighted!!!
○ The first dice for picking topics puts more weight on IT topic that on the other 5 topics.
○ Also, dice for IT topic, puts more weight on words 'linux' and 'modem'.
○ Likewise dice for topic 2 (cars) puts more weight on word 'petrol' and 'steering'


# Generative Process

![](assets/14/generative_process_plsa.png)

# Probabilistic LSA decomposition

![](assets/14/probabilistic_plsa_decomposition.png)

P(word/dodument) = SUM_topics p(topic/document)p(word/topic)

# LDA: an extension to pLSA

pLSA: Binomial distribution

LDA: [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)

# LDA Assumptions

In LDA, we encode our assumptions about the data. Two important assumptions:
1. On average, how many topics are per document? a. Few or more?
2. On average, how are words distributed across topics?
a. Are topics strongly associated with few words or not?
Those assumptions are defined by two vectors α and β:
α: K dimensional vector that defines how K topics are distributed across documents.
Smaller αs favor fewer topics strongly associated with each document.
β: V dimensional vector that defines how V words are associated across topics.
Smaller βs favor fewer words strongly associated with each topics

# LDA

We set K the number of topics

We work backwards from the observations to the find the \alpha and \beta


# Links

[Topic Modeling in historical Newspapers](http://dl.acm.org/citation.cfm?id=2107649)

http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/?_r=1

[Clustering text documents using k-means](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html)

https://github.com/alexperrier/datatalks/blob/master/twitter/LDAvis.ipynb

http://nbviewer.jupyter.org/github/alexperrier/datatalks/blob/master/twitter/LDAvis_V2.ipynb

[Dirichlet Distribution](https://www.youtube.com/watch?v=nfBNOWv1pgE)

<!-- https://github.com/alexperrier/ds-curriculum/tree/master/lessons/lesson-14 -->




