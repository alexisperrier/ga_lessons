---
layout: slide
title: Lesson 15 - Time Series
description: none
transition: slide
permalink: /15-time-series.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>15. Time series</p>
</section>

<section data-markdown>
# Previous
</section>
<section data-markdown>
# Today
</section>
<section data-markdown>
# Time series

A time series is a series of data points listed (or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.

Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, intelligent transport and trajectory forecasting,[1] earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
</section>
<section data-markdown>
# TS

* Market
* IoT
* Economics

* Forecasting
* signal detection and estimation
* Detection of a change in the parameters of a static or dynamic system

### Time vs Frequency
</section>
<section data-markdown>
# Loading and indexing in Pandas

We will work on the Dow-Jones Time series

Simple loading

        df = pd.read_csv('../data/Dow-Jones.csv', parse_dates = ['Date'], infer_datetime_format = True)
        df[df.Date > '2010-01-01']

Make the date the index:
        ts = pd.read_csv('../data/Dow-Jones.csv', parse_dates=['Date'], index_col='Date', infer_datetime_format = True)
        ts['2010-12-31':'2010-01-01']
        ts[:'2010-01-01']

</section>
<section data-markdown>
# Manipulation: Smoothing - Rolling mean - Moving Average

        ts.Value.rolling(window=7).mean()


 If those prices are p M , p M − 1 , … , p M − ( n − 1 ) {\displaystyle p_{M},p_{M-1},\dots ,p_{M-(n-1)}} p_{M},p_{M-1},\dots ,p_{M-(n-1)} then the formula is

    S M A = p M + p M − 1 + ⋯ + p M − ( n − 1 ) n = 1 n ∑ i = 0 n − 1 p M − i {\displaystyle {\begin{aligned}SMA&={\frac {p_{M}+p_{M-1}+\cdots +p_{M-(n-1)}}{n}}\\&={\frac {1}{n}}\sum _{i=0}^{n-1}p_{M-i}\end{aligned}}} {\displaystyle {\begin{aligned}SMA&={\frac {p_{M}+p_{M-1}+\cdots +p_{M-(n-1)}}{n}}\\&={\frac {1}{n}}\sum _{i=0}^{n-1}p_{M-i}\end{aligned}}}

</section>
<section data-markdown>
# Exponential weighted moving average

Introduce a Decay

![](assets/15/exponential_moving_average_weights.png)

The EMA for a series Y may be calculated recursively:

    S 1 = Y 1 {\displaystyle S_{1}=Y_{1}} S_{1}=Y_{1}

for t > 1 ,     S t = α ⋅ Y t + ( 1 − α ) ⋅ S t − 1 {\displaystyle t>1,\ \ S_{t}=\alpha \cdot Y_{t}+(1-\alpha )\cdot S_{t-1}} t>1,\ \ S_{t}=\alpha \cdot Y_{t}+(1-\alpha )\cdot S_{t-1}

Where:

    The coefficient α represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher α discounts older observations faster.
    Yt is the value at a time period t.
    St is the value of the EMA at any time period t.

http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.ewma.html
</section>

<section data-markdown>

# Auto Correlation
how correlated a variable is with itself
Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series.

pandas autocorrelation_plot() function
</section>

<section data-markdown>

# White noise
White noise
Time series that show no autocorrelation are called "white noise". Figure 2.11 gives an example of a white noise series.

white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance;

A necessary (but, in general, not sufficient) condition for statistical independence of two variables is that they be statistically uncorrelated; that is, their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element Rii is the variance of component wi; and the correlation matrix must be the n by n identity matrix.

Being uncorrelated in time does not restrict the values a signal can take. Any distribution of values is possible (although it must have zero DC component). Even a binary signal which can only take on the values 1 or -1 will be white if the sequence is statistically uncorrelated. Noise having a continuous distribution, such as a normal distribution, can of course be white.

It is often incorrectly assumed that Gaussian noise (i.e., noise with a Gaussian amplitude distribution — see normal distribution) necessarily refers to white noise, yet neither property implies the other. Gaussianity refers to the probability distribution with respect to the value, in this context the probability of the signal falling within any particular range of amplitudes, while the term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies.

We can therefore find Gaussian white noise, but also Poisson, Cauchy, etc. white noises. Thus, the two words "Gaussian" and "white" are often both specified in mathematical models of systems. Gaussian white noise is a good approximation of many real-world situations and generates mathematically tractable models. These models are used so frequently that the term additive white Gaussian noise has a standard abbreviation: AWGN.

White noise is the generalized mean-square derivative of the Wiener process or Brownian motion.

E[\varepsilon _{t}]=0\,,
{\displaystyle E[\varepsilon _{t}^{2}]=\sigma ^{2}\,,} E[\varepsilon _{t}^{2}]=\sigma ^{2}\,,
{\displaystyle E[\varepsilon _{t}\varepsilon _{s}]=0\quad \forall t\not =s\,.} {\displaystyle E[\varepsilon _{t}\varepsilon _{s}]=0\quad \forall t\not =s\,.}


usually we assume the samples all follow a Gaussian distribution
samples = numpy.random.normal(mean, std, size=num_samples)


# Resampling
http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html

# Statsmodels
http://statsmodels.sourceforge.net/stable/tsa.html

# Properties: Stationarity
A time serie is said to be stationnary if

* The mean of the series should not be a function of time rather should be a constant.
![](assets/15/Mean_nonstationary.png)

* Homoscedasticity: The variance of the series should not a be a function of time.
![](assets/15/Var_nonstationary.png)

* The covariance of the i th term and the (i + m) th term should not be a function of time.
![](assets/15/Cov_nonstationary.png)

A lot of nice results which holds for independent random variables (law of large numbers, central limit theorem to name a few) hold for stationary random variables (we should strictly say sequences). And of course it turns out that a lot of data can be considered stationary, so the concept of stationarity is very important in modeling non-independent data.

# Stationarity test

Dickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary. Refer this article for details.



# Dickey Fuller test

test Stationarity

# How to make a TS stationary
There are 2 major reasons behind non-stationaruty of a TS:
1. Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.
2. Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.

so remove trend and seasonality
and apply prediction on resulting TS

# Properties: Seasonality

# Trend

Estimating & Eliminating Trend

One of the first tricks to reduce trend can be transformation. For example, in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. These can be taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:

remove moving average

LAB: compare moving average, boxplot, ewma

# Transformations
https://www.otexts.org/fpp/2/4

* Mathematical: Box plot
* Calendar Adjustments
    monthly milk production on a farm,

* Population adjustments
    per 1000
    you remove the effect of population changes by considering number of beds per thousand people
* Inflation adjustments
        for money related series
         a price index is used. If ztzt denotes the price index and ytyt denotes the original house price in year tt, then xt=yt/zt∗z2000xt=yt/zt∗z2000 gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).


# Differencing
Differencing – taking the differece with a particular time lag

One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity.

# Decomposing
both trend and seasonality are modeled separately
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()

<!-- Part II: Modeling -->
# Naive Method
Naïve method
This method is only appropriate for time series data. All forecasts are simply set to be the value of the last observation. That is, the forecasts of all future values are set to be yTyT, where yTyT is the last observed value. This method works remarkably well for many economic and financial time series.

# Drift method
Drift method
A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. So the forecast for time T+hT+h is given by

yT+hT−1∑t=2T(yt−yt−1)=yT+h(yT−y1T−1).
yT+hT−1∑t=2T(yt−yt−1)=yT+h(yT−y1T−1).
This is equivalent to drawing a line between the first and last observation, and extrapolating it into the future.

# AR
# MA
# ARMA
# ARIMA

# De treding
# De seasonality
# Predicting
# Cross validation

Training and tests sets
# Forecast accuracy
https://www.otexts.org/fpp/2/5

Mean absolute error: MAERoot mean squared error: RMSE=mean(|ei|),=mean(e2i)‾‾‾‾‾‾‾‾‾√.
Mean absolute error: MAE=mean(|ei|),Root mean squared error: RMSE=mean(ei2).

Mean absolute percentage error: MAPE
sMAPE
MASE

# one-step forecasts may not be as relevant as multi-step forecasts

# Residuals
https://www.otexts.org/fpp/2/6
A good forecasting method will yield residuals with the following properties:

The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

The residuals have constant variance.
The residuals are normally distributed.

Histograms of residuals
QQ plots
ACF of the residuals
Box-Pierce test
Ljung-Box

# Prediction intervals
https://www.otexts.org/fpp/2/7

https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.tsplot.html

# Links

* [fecon235 : Computational data tools for financial economics](https://github.com/rsvp/fecon235)
* [Seasonal ARIMA with Python](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/)
* [Complete guide to create a Time Series Forecast ](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)
* [A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)
* [A Simple Time Series Analysis Of The S&P 500 Index](http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/)
* [Identifying the order of differencing in an ARIMA model](http://people.duke.edu/~rnau/411arim2.htm)