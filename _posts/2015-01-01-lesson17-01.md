---
layout: slide
title: Lesson 17 - Time Series Modeling
description: none
transition: slide
permalink: /17-time-series-forecast.html
theme: ga
---
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>17. Time Series Modeling and Forecasting</p>
</section>

<section data-markdown>
# Previously

* Time series
* Smoothing
* Trending and seasonality
* Stationarity, Dickey-Fuller test
* Autocorrelation, partial auto correlation
* Forecasting 101
* Metrics

</section>

<section data-markdown>
# All types of TS

* White noise

* Trend

* Seasonality

* Cycle

</section>

<section data-markdown>
# Stationarity

Required for most models.

* Mean is constant
* Variance is constant
* Autocorrelation is sliding

</section>

<section data-markdown>
# Stationarity
![](assets/17/stationary_time_series.png)
</section>

<section data-markdown>
# Testing for stationarity

### Dickey-Fuller test

**Null hypothesis**: TS is NOT stationary

Demo in Notebook
</section>

<section data-markdown>
# (Partial) Autocorrelation

### ACF
Correlation between \\( Y\_t \\) and  \\( Y\_{t-s} \\)


### PACF

Correlation between \\( Y\_t \\) and  \\( Y\_{t-s} \\)  without the added correlation between \\( Y\_t \\) and \\( Y\_{t-1} \cdots Y\_{t-s+1}  \\)

</section>


<section data-markdown>
# Stationarity

* Dickey Fuller test does not test for seasonality stationarity
* Seasonality != Cycle

*Some cases can be confusing — a time series with cyclic behaviour (but not trend or seasonality) is NOT stationary. That is because the cycles are not of fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.*


</section>

<section data-markdown>
# Simple Forecasting

* next sample = last sample \\( \hat{Y}\_{t+1} = Y\_{t}  \\)
* Moving average \\( \hat{Y}\_{t+1} = \frac{1}{n} \sum^{n}\_{i = 0} Y\_{t-1-i}  \\)
* EWMA \\(      t>1, \ \ \hat{Y}\_{t}= \alpha \cdot Y\_{t}+(1-\alpha )\cdot \hat{Y}\_{t-1} \\)
* Linear Regression OLS

</section>

<section data-markdown>
# Today

* Transform TS  into a Stationary TS
* Test is the TS predictable? Is it white noise?
* Decomposition
* Is my forecast reliable?
* Is the DJ a Random Walk?
* AutoRegressive modeling (AR) and Moving Average (MA)
* AutoRegressive Modeling Integrated Moving Average (ARIMA)

</section>

<section data-markdown>
# White noise
http://stats.stackexchange.com/questions/139869/white-noise-acf-pacf

What is white noise? Time series data that shows no auto correlation is called white noise.

How do I know whether it is white noise? A general assumption is that if 95% of the spikes in the Auto-correlation Function lie within (+/-)2/sqrt(T), where T being the length of the time series.

So, by the above mentioned formula, one can infer whether the series/data is white noise or not.

</section>

<section data-markdown>
# ACF and PACF to detect white noise
http://stats.stackexchange.com/questions/139869/white-noise-acf-pacf

</section>

<section data-markdown>
# testing for white noise
http://stats.stackexchange.com/questions/18135/testing-normality-and-independence-of-time-series-residuals

Ljung-Box test based on
Q∗=T(T+2)∑k=1h(T−k)−1r2k.

</section>


<section data-markdown>
# Residual diagnostics on forecasting
A good forecasting method will yield residuals with the following properties:

* The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
* The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

https://www.otexts.org/fpp/2/6

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

* The residuals have constant variance.
* The residuals are normally distributed.

These two properties make the calculation of prediction intervals easier

</section>




<section data-markdown>
# Train Test and Cross validation
https://www.otexts.org/fpp/2/5

</section>

<section data-markdown>
# Prediction Intervals

95% prediction interval: \\( \hat{Y\_{t}}  \pm 1.96 \sigma^2 \\)

with \\(\sigma \\)  an estimate of the standard deviation of the forecast distribution

When the residuals are normally distributed and uncorrelated and when forecasting one-step ahead
, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals.

When conditions are not met, more complex ways to estimate confidence intervals

Prediction intervals with Seaborn

</section>

<section data-markdown>
# IBM dataset
 Consider the daily closing IBM stock prices (data set ibmclose).

https://datamarket.com/data/set/2322/ibm-common-stock-closing-prices-daily-17th-may-1961-2nd-november-1962#!ds=2322&display=line

    Produce some plots of the data in order to become familiar with it.
    Split the data into a training set of 300 observations and a test set of 69 observations.
    Try various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

</section>

<section data-markdown>
# LAB: House sales
https://datamarket.com/data/set/22q8/monthly-sales-of-new-one-family-houses-sold-in-th-e-usa-since-1973#!ds=22q8&display=line

Consider the sales of new one-family houses in the USA, Jan 1973 – Nov 1995 (data set hsales).

    Produce some plots of the data in order to become familiar with it.
    remove seasonality
    diff n-1 and n-2
    Split the hsales data set into a training set and a test set, where the test set is the last two years of data.
    Try various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

</section>


<section data-markdown>
# TS Decomposition

Trend: double MA smoothing

        import statsmodels.api as sm

        dta = sm.datasets.co2.load_pandas().data
        # deal with missing values. see issue
        dta.co2.interpolate(inplace=True)

        res = sm.tsa.seasonal_decompose(dta.co2)
        resplot = res.plot()

https://gist.github.com/balzer82/5cec6ad7adc1b550e7ee
http://michaelpaulschramm.com/simple-time-series-trend-analysis/

</section>



<section data-markdown>
# AR(p) model
 In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself.

Thus an autoregressive model of order pp can be written as

yt=c+ϕ1yt−1+ϕ2yt−2+⋯+ϕpyt−p+et,

where cc is a constant and etet is white noise.

</section>

<section data-markdown>
# Special cases
For an AR(1) model:

    When ϕ1=0ϕ1=0, ytyt is equivalent to white noise.
    When ϕ1=1ϕ1=1 and c=0c=0, ytyt is equivalent to a random walk.
    When ϕ1=1ϕ1=1 and c≠0c≠0, ytyt is equivalent to a random walk with drift
    When ϕ1<0ϕ1<0, ytyt tends to oscillate between positive and negative values.

</section>
<section data-markdown>
# MA(q) models
Rather than use past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.

yt=c+et+θ1et−1+θ2et−2+⋯+θqet−q,
yt=c+et+θ1et−1+θ2et−2+⋯+θqet−q,

where etet is white noise. We refer to this as an MA(qq) model.

</section>
<section data-markdown>
# Estimating p, d, q

The squirrel approach

http://people.duke.edu/~rnau/411arim3.htm

The ML approach: Brute Force and Grid Search


</section>
<section data-markdown>
# Lab on sunspots
https://bicorner.com/2015/11/16/time-series-analysis-using-ipython/

</section>

<section data-markdown>

# Forecast with decomposition

* forecast seasonality, trend and residuals separately
* add back together


</section>
<section data-markdown>
# LAB Is the DJ a random walk?
Why you cannot beat the market

</section>
<section data-markdown>
# What's a Random Walk
When the differenced series is white noise, the model for the original series can be written as

yt−yt−1=etoryt=yt−1+et:.
yt−yt−1=etoryt=yt−1+et:.

A random walk model is very widely used for non-stationary data, particularly finance and economic data. Random walks typically have:

    long periods of apparent trends up or down
    sudden and unpredictable changes in direction.


http://python-for-signal-processing.blogspot.com/2014/04/random-walks-and-stumbles.html



</section>

<section data-markdown>

# Notebook: Proof that the DJ is a random walk
http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/

* plot DJ
* plot diff
* transofrm with log
* plot rolling variance original + log
* plot diff of log => stationary time series model of daily changes to the S&P 500 index
* lag variables scatter plot => all centered and normal
* acf and pacf => no correlation => increment is white noise => we have a random walk
* decomposition => look at the residuals white noise ?
* AR model, look at the residuals => much smaler valuespredicted than actual changes


* look at histogram of residuals
    * skewed => not great for confidence intervals
* autocorrelation plot of residuals
* test with Ljung-Box



</section>



<section data-markdown>

# Dickey Fuller
http://stats.stackexchange.com/questions/44647/which-dickey-fuller-test-should-i-apply-to-a-time-series-with-an-underlying-mode

DF on stationarity and seasonality

http://stats.stackexchange.com/questions/225087/seasonal-data-deemed-stationary-by-adf-and-kpss-tests

# Random Walk
http://python-for-signal-processing.blogspot.com/2014/04/random-walks-and-stumbles.html

http://fedc.wiwi.hu-berlin.de/xplore/tutorials/xegbohtmlnode39.html
</section>