---
layout: slide
title: Lesson 06 - Linear regression in Scikit
description: none
transition: slide
permalink: /06-scikit-linear-regression.html
theme: ga
---
<section >
<h1>Hello </h1>

with <span> \( \alpha \) </span> mathjax

$$ \alpha $$

</section>

<!-- * Scikit
* linear_models in scikit: Ridge, Lasso, Elastic
* Linear regression review: p_values, R-Squared, confidence intervals
* Multicollinearity
* Polynomial regression using scikit-learn

* math basis for linear regression
* Loss function, Residual error, Mean Squared Error (MSE)
 -->
<section  data-background-color="#000">
    <h1 class = 'white' style ="border-top: thin solid #DDD;border-bottom: thin solid #DDD;">
        <img src="assets/ga_logo_black.png" style="float:left;top:0px;">
        General Assembly
    </h1>
    <p class = 'big_title'>Linear regression in Scikit</p>
</section>

<section data-markdown>
# Title
## LEARNING OBJECTIVES

* Scikit-learn
* Math basis for linear regression
* Loss function, Residual error, Mean Squared Error (MSE)
* linear_models in scikit: Ridge, Lasso, Elastic
* Linear regression review: p_values, R-Squared, confidence intervals
* Multicollinearity
* Polynomial regression using scikit-learn
</section>

<!-- Prework and review -->
<section  data-background-color="#DA0A13">
    <h1>Lesson #N</h1>
    <p class = 'big_title'>Pre Work & Review</p>
</section>

<section data-markdown>
# Prework


</section>

<section data-markdown>
# Last Lesson Review

* Matplotlib
* Seaborn
* Bokeh, Seaborn, Plot.ly

</section>

<section data-markdown>
# Last session
### Any questions from last class?
Note:
Exit tickets
</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Today</h1>
    <p class = 'big_title'>Scikit-learn  </p>
</section>


<section data-markdown>
# Scikit-learn - Overview

Classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

As of 2016, scikit-learn is under active development and is sponsored by INRIA, Telecom ParisTech

Scikit-learn is largely written in Python, with some core algorithms written in Cython

http://scikit-learn.org

What makes scikit-learn so popular is

* Excellent and extensive algorithm implementation
* Simple conceptual API
* Fantastic documentation
* Super Efficient workflow

</section>

<section data-markdown>
# Scikit-learn - Overview

* Classification: Identifying to which category an object belongs to.
    * Applications: Spam detection, Image recognition.
    * Algorithms: SVM, nearest neighbors, random forest, ...

* Regression: Predicting a continuous-valued attribute associated with an object.
    * Applications: Drug response, Stock prices.
    *  Algorithms: SVR, ridge regression, Lasso, ...

* Clustering: Automatic grouping of similar objects into sets.
    * Applications: Customer segmentation, Grouping experiment outcomes
    * Algorithms: k-Means, spectral clustering, mean-shift, ...

* Dimensionality reduction: Reducing the number of random variables to consider.
    * Applications: Visualization, Increased efficiency
    * Algorithms: PCA, feature selection, non-negative matrix factorization.

* Model selection: Comparing, validating and choosing parameters and models.
    * Goal: Improved accuracy via parameter tuning
    * Modules: grid search, cross validation, metrics.

* Preprocessing: Feature extraction and normalization.
    * Application: Transforming input data such as text for use with machine learning algorithms.
    * Modules: preprocessing, feature extraction.

</section>

<section data-markdown>
# Scikit-learn - API

Could not be simpler

1. select a model
for instance
        regr = linear_model.LinearRegression( some model tuning params, the metric, ...)
or

2. Train the model to the data
        regr.fit(X, y)

3. Predict on new data
        y_hat = regr.predict(Some New Data)

4. Score
        regr.score(X,y)

</section>

<!-- Today -->
<section  data-background-color="#22c8c6">
    <h1>Linear regression </h1>
    <p class = 'big_title'>The underlying math</p>
    <p class = 'big_title'>Loss function</p>
</section>

<section data-markdown>
# Hello

$$ \alpha $$

</section>

<section data-markdown>
# Ordinary least squares (OLS)



Given X and y you want to find the best function f that minimizes
    $$ ||y - f(X)||^2 $$


Ordinary least squares (OLS) is the simplest and thus most common estimator. It is conceptually simple and computationally straightforward.

OLS estimates are commonly used to analyze both experimental and observational data.
The OLS method minimizes the sum of squared residuals, and leads to a closed-form expression for the estimated value of the unknown parameter.

The estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors

</section>
<section data-markdown>
# OLS

Let's say we have \\( n  \\) samples:

* Variable \\( x = [x_1, ... , x_n]  \\)

* Outcome \\( y = [y_1, ... , y_n]  \\)

We want to find the *best* a and b such that for all \\( x_i  \\) and \\( y_i  \\)

$$ y_i = a * x_i +b $$

Would work perfectly of the points \\(  (x_i, y_i) \\) were on a line.
But they are not!

so even for the best \\( a  \\) and \\( b  \\) we'll end up with

$$ \hat{y_i} = a * x_i +b $$

with \\( \hat{y_i} \neq y_i  \\)
</section>
<section data-markdown>
# Residuals

The residuals are

$$ \epsilon_i = y_i - \hat{y_i} $$
$$ \epsilon_i = y_i - (a * x_i +b) $$

for  \\( i = [1, ... , n]  \\)

The residuals are the **distance** between the **true** outcomes  \\( y_i  \\)  and their estimates  \\( \hat{y_i}  \\)

We want to minimize the distance
</section>

<section data-markdown>
# Loss function

We do so by minimizing the  \\( L^2  \\) norm of the residuals

$$  || y - \hat{y} || =  || y - (ax +b) || $$


$$  || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$

</section>

<section data-markdown>

# Norms
###  \\( L^2  \\) norm
$$  ||x|| = \sqrt{  x_1^2 + .... + x_n^2  } $$

###  \\( L^1  \\) norm
$$  |x| =  |x_1| + .... + |x_n|  $$

###  \\( L^\infty  \\) norm

$$  |x|_{\infty} =  max [ |x_1|, ... , |x_n| ]  $$

</section>

<section data-markdown>
# Loss function

$$ L(a,b)  = || y - \hat{y} ||^2 =  \sum_{i=0}^n [y_i - (a*x_i + b)]^2   $$

* **Convex** function
* to find the minimum, set the derivative over a and b to \\( 0 \\)
* that gives us 2 linear equations in a and b that we can solve

[Wikipedia example](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))

</section>

<section data-markdown>
# General case

* \\( m\\) samples \\( y = [y_1, ... , y_n]  \\)
* \\( n\\) regressors / variables



$$ X = \[ (x_{i,j}) \]  $$

X is an (m by n) matrix

You want to find the weights \\( \beta = [\beta_1, ...., \beta_n] \\) that minimizes

$$  L(\beta) = || y - X\beta ||  $$

The solution is

$$ \hat{\beta} =  (X^T.X)^{-1} X^T y  $$

</section>
<section data-markdown>
# Let's python that

* 4 samples

        input = np.array([ [1, 6], [2, 5], [3, 7], [4, 10] ])

* Matrix of predictors

        X = np.matrix([np.ones(m), input[:,0]]).T

* Outcome vector

        y = np.matrix(input[:,1]).T

* estimated regression weights \\( \hat{\beta} = (X^T . X)^{-1} X^T y \\)

        beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

* and plot it
</section>


<section data-markdown>
# Scikit-learn - Overview
### [Generalized Linear Models](http://scikit-learn.org/stable/modules/linear_model.html)



http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#example-linear-model-plot-ols-py

</section>





<section data-markdown>
# Hello
1. markdown

with  \\( \alpha \\)  mathjax
$$ \sum_i^n x_i$$

</section>

<section >
<h1>Hello </h1>

with <span> \( \alpha \) </span> mathjax
<div>
$$ \sum_i^n x_i$$
</div>
</section>

